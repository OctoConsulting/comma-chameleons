{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Certainly, tailoring a Workforce Resilience Program specifically for employees of the Office of Information Technology (OIT) at the Centers for Medicare & Medicaid Services (CMS) means integrating elements pertinent to the nature of their work, the demands of the healthcare sector, and the unique challenges faced by IT professionals. Here's how this could be done:\\n\\n### CMS OIT Workforce Resilience Program\\n\\n1. **Program Objective**:\\n    - Support the continuous operation of CMS digital infrastructure and health data systems.\\n    - Align with healthcare regulations and data protection standards.\\n    - Foster a resilient IT community capable of responding to healthcare crises.\\n\\n2. **Risk Assessment**:\\n    - Identify IT-specific challenges, such as cybersecurity threats, system outages, and data breaches.\\n    - Evaluate potential health sector crises like pandemics, which may strain digital services.\\n\\n3. **Training & Education**:\\n    - Enhance cybersecurity awareness and promote best practices.\\n    - Conduct workshops on healthcare IT trends and emerging technologies.\\n    - Offer courses on patient data privacy and HIPAA compliance.\\n\\n10. **Leadership & Management Training**:\\n    - Focus on IT project management, agile methodologies, and stakeholder communication.\\n    - Emphasize healthcare technology innovation and its leadership.\\n\\n14. **Continuous Improvement**:\\n    - Stay updated with advances in healthcare IT.\\n    - Engage in public-private partnerships for innovation in health technology.\\n\\nFor CMS OIT, ensuring resilience isn't just about keeping systems running; it's about ensuring that millions of beneficiaries continue receiving the care and support they need. A resilience program tailored to their unique demands ensures that in the face of adversity, CMS's digital backbone remains strong and responsive.\\n\\nLet's dive deep into the training aspect tailored for CMS OIT employees, focusing on the skills mentioned:\\n\\n### CMS OIT Training for Workforce Resilience\\n\\n#### 1. **Product Management (PM) Training**:\\n\\n- **Objective**: Equip OIT personnel with the ability to manage software products and projects efficiently, ensuring alignment with CMS's strategic goals.\\n  \\n- **Modules**:\\n    - Fundamentals of Healthcare IT Product Management.\\n    - Navigating regulatory and compliance requirements in product development.\\n    - Stakeholder communication and management for IT projects.\\n\\n#### 2. **Human-Centered Design (HCD)**:\\n\\n- **Objective**: Foster a user-first approach in designing and deploying IT solutions within CMS.\\n  \\n- **Modules**:\\n    - Introduction to Human-Centered Design in Healthcare.\\n    - Conducting user research within CMS's ecosystem.\\n    - Iterative design and prototyping for healthcare applications.\\n\\n#### 3. **Data Science Training**:\\n\\n- **Objective**: Enable OIT staff to analyze and derive insights from vast amounts of healthcare data, driving data-informed decision-making.\\n  \\n- **Modules**:\\n    - Data exploration, cleaning, and visualization tailored to healthcare datasets.\\n    - Statistical modeling and hypothesis testing in healthcare.\\n    - Advanced analytics techniques for patient data.\\n\\n#### 4. **Machine Learning (ML)**:\\n\\n- **Objective**: Equip OIT teams with the skills to develop predictive models for improved healthcare service delivery and fraud detection.\\n  \\n- **Modules**:\\n    - Machine Learning basics and its applications in healthcare.\\n    - Supervised and unsupervised learning for patient data analysis.\\n    - Deep learning for medical imaging and diagnostics.\\n\\n#### 5. **Cybersecurity (Cyber) Training**:\\n\\n- **Objective**: Strengthen CMS's digital defenses, ensuring the confidentiality, integrity, and availability of health data.\\n  \\n- **Modules**:\\n    - Fundamentals of cybersecurity in the healthcare sector.\\n    - Threat detection, prevention, and response in CMS's IT landscape.\\n    - Ensuring HIPAA compliance in cybersecurity practices.\\n\\n#### 6. **Cloud Computing Training**:\\n\\n- **Objective**: Facilitate OIT personnel's understanding of cloud services, ensuring efficient, scalable, and secure digital solutions for CMS.\\n  \\n- **Modules**:\\n    - Introduction to cloud computing in healthcare.\\n    - Designing and deploying healthcare applications on the cloud.\\n    - Ensuring data security and compliance in a cloud environment.\\n\\n#### Delivery and Evaluation:\\n\\n- **Blended Learning Approach**: Combine in-person training, online courses, workshops, and hands-on projects for a holistic learning experience.\\n  \\n- **Expert-led Sessions**: Engage industry experts and internal thought leaders to impart real-world knowledge and experiences.\\n  \\n- **Assessments**: Periodic assessments to gauge understanding, retention, and application of the skills learned.\\n  \\n- **Feedback Loop**: Allow participants to provide feedback after each module, ensuring continuous improvement in content and delivery.\\n\\n#### Continuous Skill Upgradation:\\n\\nGiven the rapidly evolving nature of technology and healthcare, it's essential to ensure that CMS OIT employees have access to regular updates and advanced courses in these areas.\\n\\nThis tailored training program ensures that CMS OIT employees are equipped with the most relevant and up-to-date skills, enhancing their ability to deliver efficient, secure, and user-friendly solutions to beneficiaries and stakeholders.\\n\\n\\nAbsolutely, incorporating the pillars of Awareness, Competency, and Engagement adds depth and structure to the training program. Letï¿½s refine the CMS OIT Training for Workforce Resilience under these pillars:\\n\\n### Pillar 1: Awareness\\n\\nCreating an environment where employees are informed about the significance, applications, and potential of the mentioned skill areas in their domain.\\n\\n#### 1. **Product Management**:\\n- **Awareness Modules**: Introduction to the role of Product Management in healthcare IT, its significance in CMS, and the impact of good PM practices on patient care and service efficiency.\\n\\n#### 2. **Human-Centered Design**:\\n- **Awareness Modules**: Understanding the patient and stakeholder perspective, the value of user-centric designs in healthcare applications, and case studies on the impact of HCD in healthcare outcomes.\\n\\n#### 3. **Data Science**:\\n- **Awareness Modules**: The significance of data-driven decisions in healthcare, potential of data insights in CMS operations, and real-world impacts of data analytics in patient care.\\n\\n#### 4. **Machine Learning**:\\n- **Awareness Modules**: Demystifying ML in healthcare, its potential applications (from predictive health outcomes to fraud detection), and its role in future healthcare paradigms.\\n\\n#### 5. **Cybersecurity**:\\n- **Awareness Modules**: The importance of cyber hygiene in healthcare, potential threats and their impacts, and real-world incidents of cybersecurity breaches in healthcare institutions.\\n\\n#### 6. **Cloud Computing**:\\n- **Awareness Modules**: The role of cloud in modern healthcare IT, benefits like scalability and efficiency, and the transformational potential of cloud applications in healthcare.\\n\\n### Pillar 2: Competency\\n\\nEnsuring that OIT employees not only understand but are also skilled in the practical applications of their knowledge.\\n\\n#### For each of the aforementioned skills:\\n- **Competency Modules**: Hands-on exercises, workshops, and practical projects that require participants to apply their theoretical knowledge. For instance, in the case of Cybersecurity, this might involve simulated cyber-attack drills or vulnerability assessment exercises.\\n\\n### Pillar 3: Engagement\\n\\nEngaging participants in ongoing learning, fostering a sense of community, and promoting the real-world application of skills.\\n\\n#### For each of the aforementioned skills:\\n**Engagement Activities**:\\n    - **Knowledge Sharing Sessions**: Regular meetups where participants discuss challenges, share experiences, and present on recent advances or projects they've worked on.\\n    - **Communities of Practice**: Creating and fostering communities within CMS OIT where professionals interested in specific areas (like ML or Cloud Computing) can collaborate, share resources, and mentor each other.\\n    - **Continual Learning Opportunities**: Offering advanced courses, sending updates on the latest trends, and providing access to industry conferences or seminars.\\n    - **Feedback Platforms**: Tools or platforms where participants can offer feedback, ask questions, or seek further clarifications on their training modules.\\n\\nBy incorporating these three pillars - Awareness, Competency, and Engagement - the training program becomes a holistic initiative, ensuring that CMS OIT employees are not just trained but are also empowered, engaged, and invested in their learning and its application.\\n\\nAbsolutely, here's a structured breakdown of the CMS OIT Training for Workforce Resilience using the given time frames for the pillars of Awareness, Competency, and Engagement:\\n\\n### Pillar 1: Awareness\\n\\nProviding foundational understanding and insights into each domain, totaling a 3-hour commitment. Participants can choose from two delivery modes:\\n\\n- **Weekly Insight Sessions**: 1 hour per week spread across 3 weeks.\\n- **Concentrated Bootcamp**: A comprehensive 3-hour session covering all areas.\\n\\n#### For each skill:\\n\\n- Modules would be concise, emphasizing the significance, applications, and potential in the CMS OIT domain. They'd cover core concepts, real-world applications, and implications within the healthcare IT context.\\n\\n### Pillar 2: Competency\\n\\nBuilding hands-on proficiency with a structured 12-hour program spread across 6 weeks.\\n\\n- **Weekly Skill-Deepening Workshops**: 2 hours per week for 6 weeks.\\n\\n#### For each of the mentioned skills:\\n\\n- Participants will delve deeper into each area, practicing their knowledge through exercises, simulated projects, and practical scenarios. For instance, in Data Science, they might be tasked with analyzing real-world healthcare datasets and deriving actionable insights, while in Cybersecurity, they could work on identifying vulnerabilities in simulated IT environments.\\n\\n### Pillar 3: Engagement\\n\\nPromoting continual learning, collaboration, and application of skills. Given its nature, this pillar is characterized by a flexible time commitment, adapting to individual and collective preferences.\\n\\n#### For each skill:\\n\\n- **Knowledge Sharing Sessions**: Periodic meetups or webinars where members discuss recent advances, challenges, and case studies.\\n- **Communities of Practice**: Encourage participation in forums and discussion groups that foster collaboration and mutual mentorship.\\n- **Continual Learning Opportunities**: Notifications and access to advanced courses, industry webinars, seminars, or conferences based on individual interest.\\n- **Feedback Platforms**: Online platforms open for feedback, queries, and clarifications round the clock, ensuring support is available as and when required.\\n\\nIncorporating structured time frames for each pillar ensures that participants can plan their involvement effectively, balancing their training commitments with their routine tasks. The split across Awareness, Competency, and Engagement ensures that learning is not just an event but an ongoing journey, tailored to accommodate the demanding schedules of CMS OIT employees.\\n\", metadata={'source': 'docs/FakeWR.txt'})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader('docs/FakeWR.txt')\n",
    "docs = loader.load()\n",
    "len(docs)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully fetched the webpage.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#web scrape TTB.gov\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "docs = []\n",
    "url = 'https://www.ttb.gov/beer/labeling/malt-beverage-mandatory-label-information'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully fetched the webpage.\")\n",
    "else:\n",
    "    print(\"Failed to fetch the webpage.\")\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#title = soup.find('title').get_text()\n",
    "source = soup.find('base').attrs['href']\n",
    "#docs[0] = soup.getText()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "#url = \"https://docs.python.org/3.9/\"\n",
    "url = \"https://js.langchain.com/docs/modules/memory/\"\n",
    "# max_depth=2,extractor=lambda x: Soup(x, \"html.parser\").text\n",
    "loader = RecursiveUrlLoader(url=url)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nExamples: Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsExamples: Memoryðï¸ DynamoDB-Backed Chat MemoryFor longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a DynamoDB instance.ðï¸ Firestore Chat MemoryFor longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a firestore.ðï¸ Momento-Backed Chat MemoryFor distributed, serverless persistence across chat sessions, you can swap in a Momento-backed chat message history.ðï¸ MongoDB Chat MemoryFor longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a MongoDB instance.ðï¸ MotÃ¶rhead MemoryMotÃ¶rhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.ðï¸ PlanetScale Chat MemoryBecause PlanetScale works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.ðï¸ Redis-Backed Chat MemoryFor longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Redis instance.ðï¸ Upstash Redis-Backed Chat MemoryBecause Upstash Redis works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.ðï¸ Xata Chat MemoryXata is a serverless data platform, based on PostgreSQL. It provides a type-safe TypeScript/JavaScript SDK for interacting with your database, and aðï¸ Zep MemoryZep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.PreviousVector store-backed memoryNextDynamoDB-Backed Chat MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/', 'title': 'Examples: Memory | ð¦ï¸ð Langchain', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nMomento-Backed Chat Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsMomento-Backed Chat MemoryMomento-Backed Chat MemoryFor distributed, serverless persistence across chat sessions, you can swap in a Momento-backed chat message history.\\nBecause a Momento cache is instantly available and requires zero infrastructure maintenance, it\\'s a great way to get started with chat history whether building locally or in production.Setup\\u200bYou will need to install the Momento Client Library in your project:npmYarnpnpmnpm install @gomomento/sdkyarn add @gomomento/sdkpnpm add @gomomento/sdkYou will also need an API key from Momento. You can sign up for a free account here.Usage\\u200bTo distinguish one chat history session from another, we need a unique sessionId. You may also provide an optional sessionTtl to make sessions expire after a given number of seconds.import {  CacheClient,  Configurations,  CredentialProvider,} from \"@gomomento/sdk\";import { BufferMemory } from \"langchain/memory\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";import { MomentoChatMessageHistory } from \"langchain/stores/message/momento\";// See https://github.com/momentohq/client-sdk-javascript for connection optionsconst client = new CacheClient({  configuration: Configurations.Laptop.v1(),  credentialProvider: CredentialProvider.fromEnvironmentVariable({    environmentVariableName: \"MOMENTO_AUTH_TOKEN\",  }),  defaultTtlSeconds: 60 * 60 * 24,});// Create a unique session IDconst sessionId = new Date().toISOString();const cacheName = \"langchain\";const memory = new BufferMemory({  chatHistory: await MomentoChatMessageHistory.fromProps({    client,    cacheName,    sessionId,    sessionTtl: 300,  }),});console.log(  `cacheName=${cacheName} and sessionId=${sessionId} . This will be used to store the chat history. You can inspect the values at your Momento console at https://console.gomomento.com.`);const model = new ChatOpenAI({  modelName: \"gpt-3.5-turbo\",  temperature: 0,});const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/// See the chat history in the Momentoconsole.log(await memory.chatHistory.getMessages());API Reference:BufferMemory from langchain/memoryChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsMomentoChatMessageHistory from langchain/stores/message/momentoPreviousFirestore Chat MemoryNextMongoDB Chat MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/momento', 'title': 'Momento-Backed Chat Memory | ð¦ï¸ð Langchain', 'description': 'For distributed, serverless persistence across chat sessions, you can swap in a Momento-backed chat message history.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nRedis-Backed Chat Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsRedis-Backed Chat MemoryRedis-Backed Chat MemoryFor longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Redis instance.Setup\\u200bYou will need to install node-redis in your project:npmYarnpnpmnpm install redisyarn add redispnpm add redisYou will also need a Redis instance to connect to. See instructions on the official Redis website for running the server locally.Usage\\u200bEach chat history session stored in Redis must have a unique id. You can provide an optional sessionTTL to make sessions expire after a give number of seconds.\\nThe config parameter is passed directly into the createClient method of node-redis, and takes all the same arguments.import { BufferMemory } from \"langchain/memory\";import { RedisChatMessageHistory } from \"langchain/stores/message/ioredis\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";const memory = new BufferMemory({  chatHistory: new RedisChatMessageHistory({    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation    sessionTTL: 300, // 5 minutes, omit this parameter to make sessions never expire    url: \"redis://localhost:6379\", // Default value, override with your own instance\\'s URL  }),});const model = new ChatOpenAI({  modelName: \"gpt-3.5-turbo\",  temperature: 0,});const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryRedisChatMessageHistory from langchain/stores/message/ioredisChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsAdvanced Usage\\u200bYou can also directly pass in a previously created node-redis client instance:import { Redis } from \"ioredis\";import { BufferMemory } from \"langchain/memory\";import { RedisChatMessageHistory } from \"langchain/stores/message/ioredis\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";const client = new Redis(\"redis://localhost:6379\");const memory = new BufferMemory({  chatHistory: new RedisChatMessageHistory({    sessionId: new Date().toISOString(),    sessionTTL: 300,    client,  }),});const model = new ChatOpenAI({  modelName: \"gpt-3.5-turbo\",  temperature: 0,});const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryRedisChatMessageHistory from langchain/stores/message/ioredisChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsRedis Sentinel Support\\u200bYou can enable a Redis Sentinel backed cache using ioredisThis will require the installation of ioredis in your project.npmYarnpnpmnpm install ioredisyarn add ioredispnpm add ioredisimport { Redis } from \"ioredis\";import { BufferMemory } from \"langchain/memory\";import { RedisChatMessageHistory } from \"langchain/stores/message/ioredis\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";// Uses ioredis to facilitate Sentinel Connections see their docs for details on setting up more complex Sentinels: https://github.com/redis/ioredis#sentinelconst client = new Redis({  sentinels: [    { host: \"localhost\", port: 26379 },    { host: \"localhost\", port: 26380 },  ],  name: \"mymaster\",});const memory = new BufferMemory({  chatHistory: new RedisChatMessageHistory({    sessionId: new Date().toISOString(),    sessionTTL: 300,    client,  }),});const model = new ChatOpenAI({ temperature: 0.5 });const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryRedisChatMessageHistory from langchain/stores/message/ioredisChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsPreviousPlanetScale Chat MemoryNextUpstash Redis-Backed Chat MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/redis', 'title': 'Redis-Backed Chat Memory | ð¦ï¸ð Langchain', 'description': 'For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Redis instance.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nPlanetScale Chat Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsPlanetScale Chat MemoryPlanetScale Chat MemoryBecause PlanetScale works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for an PlanetScale Database instance.Setup\\u200bYou will need to install @planetscale/database in your project:npmYarnpnpmnpm install @planetscale/databaseyarn add @planetscale/databasepnpm add @planetscale/databaseYou will also need an PlanetScale Account and a database to connect to. See instructions on PlanetScale Docs on how to create a HTTP client.Usage\\u200bEach chat history session stored in PlanetScale database must have a unique id.\\nThe config parameter is passed directly into the new Client() constructor of @planetscale/database, and takes all the same arguments.import { BufferMemory } from \"langchain/memory\";import { PlanetScaleChatMessageHistory } from \"langchain/stores/message/planetscale\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";const memory = new BufferMemory({  chatHistory: new PlanetScaleChatMessageHistory({    tableName: \"stored_message\",    sessionId: \"lc-example\",    config: {      url: \"ADD_YOURS_HERE\", // Override with your own database instance\\'s URL    },  }),});const model = new ChatOpenAI();const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryPlanetScaleChatMessageHistory from langchain/stores/message/planetscaleChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsAdvanced Usage\\u200bYou can also directly pass in a previously created @planetscale/database client instance:import { BufferMemory } from \"langchain/memory\";import { PlanetScaleChatMessageHistory } from \"langchain/stores/message/planetscale\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";import { Client } from \"@planetscale/database\";// Create your own Planetscale database clientconst client = new Client({  url: \"ADD_YOURS_HERE\", // Override with your own database instance\\'s URL});const memory = new BufferMemory({  chatHistory: new PlanetScaleChatMessageHistory({    tableName: \"stored_message\",    sessionId: \"lc-example\",    client, // You can reuse your existing database client  }),});const model = new ChatOpenAI();const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryPlanetScaleChatMessageHistory from langchain/stores/message/planetscaleChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsPreviousMotÃ¶rhead MemoryNextRedis-Backed Chat MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/planetscale', 'title': 'PlanetScale Chat Memory | ð¦ï¸ð Langchain', 'description': 'Because PlanetScale works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nMotÃ¶rhead Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsMotÃ¶rhead MemoryMotÃ¶rhead MemoryMotÃ¶rhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.Setup\\u200bSee instructions at MotÃ¶rhead for running the server locally, or https://getmetal.io to get API keys for the hosted version.Usage\\u200bimport { MotorheadMemory } from \"langchain/memory\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";// Managed Example (visit https://getmetal.io to get your keys)// const managedMemory = new MotorheadMemory({//   memoryKey: \"chat_history\",//   sessionId: \"test\",//   apiKey: \"MY_API_KEY\",//   clientId: \"MY_CLIENT_ID\",// });// Self Hosted Exampleconst memory = new MotorheadMemory({  memoryKey: \"chat_history\",  sessionId: \"test\",  url: \"localhost:8080\", // Required for self hosted});const model = new ChatOpenAI({  modelName: \"gpt-3.5-turbo\",  temperature: 0,});const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:MotorheadMemory from langchain/memoryChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsPreviousMongoDB Chat MemoryNextPlanetScale Chat MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/motorhead_memory', 'title': 'MotÃ¶rhead Memory | ð¦ï¸ð Langchain', 'description': 'MotÃ¶rhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nZep Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsZep MemoryZep MemoryZep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.Key Features:Long-term memory persistence, with access to historical messages irrespective of your summarization strategy.Auto-summarization of memory messages based on a configurable message window. A series of summaries are stored, providing flexibility for future summarization strategies.Vector search over memories, with messages automatically embedded on creation.Auto-token counting of memories and summaries, allowing finer-grained control over prompt assembly.Python and JavaScript SDKs.Setup\\u200bSee the instructions from Zep for running the server locally or through an automated hosting provider.Usage\\u200bimport { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";import { ZepMemory } from \"langchain/memory/zep\";import { randomUUID } from \"crypto\";const sessionId = randomUUID(); // This should be unique for each user or each user\\'s session.const zepURL = \"http://localhost:8000\";const memory = new ZepMemory({  sessionId,  baseURL: zepURL,  // This is optional. If you\\'ve enabled JWT authentication on your Zep server, you can  // pass it in here. See https://docs.getzep.com/deployment/auth  apiKey: \"change_this_key\",});const model = new ChatOpenAI({  modelName: \"gpt-3.5-turbo\",  temperature: 0,});const chain = new ConversationChain({ llm: model, memory });console.log(\"Memory Keys:\", memory.memoryKeys);const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/console.log(\"Session ID: \", sessionId);console.log(\"Memory: \", await memory.loadMemoryVariables({}));API Reference:ChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsZepMemory from langchain/memory/zepPreviousXata Chat MemoryNextAgentsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/zep_memory', 'title': 'Zep Memory | ð¦ï¸ð Langchain', 'description': 'Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nDynamoDB-Backed Chat Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsDynamoDB-Backed Chat MemoryDynamoDB-Backed Chat MemoryFor longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a DynamoDB instance.Setup\\u200bFirst, install the AWS DynamoDB client in your project:npmYarnpnpmnpm install @aws-sdk/client-dynamodbyarn add @aws-sdk/client-dynamodbpnpm add @aws-sdk/client-dynamodbNext, sign into your AWS account and create a DynamoDB table. Name the table langchain, and name your partition key id. Make sure your partition key is a string. You can leave sort key and the other settings alone.You\\'ll also need to retrieve an AWS access key and secret key for a role or user that has access to the table and add them to your environment variables.Usage\\u200bimport { BufferMemory } from \"langchain/memory\";import { DynamoDBChatMessageHistory } from \"langchain/stores/message/dynamodb\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";const memory = new BufferMemory({  chatHistory: new DynamoDBChatMessageHistory({    tableName: \"langchain\",    partitionKey: \"id\",    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation    config: {      region: \"us-east-2\",      credentials: {        accessKeyId: \"<your AWS access key id>\",        secretAccessKey: \"<your AWS secret access key>\",      },    },  }),});const model = new ChatOpenAI();const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryDynamoDBChatMessageHistory from langchain/stores/message/dynamodbChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsPreviousExamplesNextFirestore Chat MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/dynamodb', 'title': 'DynamoDB-Backed Chat Memory | ð¦ï¸ð Langchain', 'description': 'For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a DynamoDB instance.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nUpstash Redis-Backed Chat Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsUpstash Redis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryBecause Upstash Redis works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.\\nBased on Redis-Backed Chat Memory.For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for an Upstash Redis instance.Setup\\u200bYou will need to install @upstash/redis in your project:npmYarnpnpmnpm install @upstash/redisyarn add @upstash/redispnpm add @upstash/redisYou will also need an Upstash Account and a Redis database to connect to. See instructions on Upstash Docs on how to create a HTTP client.Usage\\u200bEach chat history session stored in Redis must have a unique id. You can provide an optional sessionTTL to make sessions expire after a give number of seconds.\\nThe config parameter is passed directly into the new Redis() constructor of @upstash/redis, and takes all the same arguments.import { BufferMemory } from \"langchain/memory\";import { UpstashRedisChatMessageHistory } from \"langchain/stores/message/upstash_redis\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";const memory = new BufferMemory({  chatHistory: new UpstashRedisChatMessageHistory({    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation    sessionTTL: 300, // 5 minutes, omit this parameter to make sessions never expire    config: {      url: \"https://ADD_YOURS_HERE.upstash.io\", // Override with your own instance\\'s URL      token: \"********\", // Override with your own instance\\'s token    },  }),});const model = new ChatOpenAI({  modelName: \"gpt-3.5-turbo\",  temperature: 0,});const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryUpstashRedisChatMessageHistory from langchain/stores/message/upstash_redisChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsAdvanced Usage\\u200bYou can also directly pass in a previously created @upstash/redis client instance:import { Redis } from \"@upstash/redis\";import { BufferMemory } from \"langchain/memory\";import { UpstashRedisChatMessageHistory } from \"langchain/stores/message/upstash_redis\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";// Create your own Redis clientconst client = new Redis({  url: \"https://ADD_YOURS_HERE.upstash.io\",  token: \"********\",});const memory = new BufferMemory({  chatHistory: new UpstashRedisChatMessageHistory({    sessionId: new Date().toISOString(),    sessionTTL: 300,    client, // You can reuse your existing Redis client  }),});const model = new ChatOpenAI({  modelName: \"gpt-3.5-turbo\",  temperature: 0,});const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryUpstashRedisChatMessageHistory from langchain/stores/message/upstash_redisChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsPreviousRedis-Backed Chat MemoryNextXata Chat MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/upstash_redis', 'title': 'Upstash Redis-Backed Chat Memory | ð¦ï¸ð Langchain', 'description': 'Because Upstash Redis works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nFirestore Chat Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsFirestore Chat MemoryFirestore Chat MemoryFor longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a firestore.Setup\\u200bFirst, install the Firebase admin package in your project:npmYarnpnpmyarn add firebase-adminyarn add firebase-adminyarn add firebase-adminGo to your the Settings icon Project settings in the Firebase console.\\nIn the Your apps card, select the nickname of the app for which you need a config object.\\nSelect Config from the Firebase SDK snippet pane.\\nCopy the config object snippet, then add it to your firebase functions FirestoreChatMessageHistory.Usage\\u200bimport { BufferMemory } from \"langchain/memory\";import { FirestoreChatMessageHistory } from \"langchain/stores/message/firestore\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";const memory = new BufferMemory({  chatHistory: new FirestoreChatMessageHistory({    collectionName: \"langchain\",    sessionId: \"lc-example\",    userId: \"a@example.com\",    config: { projectId: \"your-project-id\" },  }),});const model = new ChatOpenAI();const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryFirestoreChatMessageHistory from langchain/stores/message/firestoreChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsFirestore Rules\\u200bIf your collection name is \"chathistory,\" you can configure Firestore rules as follows.      match /chathistory/{sessionId} {       allow read: if request.auth.uid == resource.data.createdBy;       allow write: if request.auth.uid == request.resource.data.createdBy;             }             match /chathistory/{sessionId}/messages/{messageId} {       allow read: if request.auth.uid == resource.data.createdBy;       allow write: if request.auth.uid == request.resource.data.createdBy;            }PreviousDynamoDB-Backed Chat MemoryNextMomento-Backed Chat MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/firestore', 'title': 'Firestore Chat Memory | ð¦ï¸ð Langchain', 'description': 'For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a firestore.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nMongoDB Chat Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsMongoDB Chat MemoryMongoDB Chat MemoryFor longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a MongoDB instance.Setup\\u200bYou need to install Node MongoDB SDK in your project:npmYarnpnpmnpm install -S mongodbyarn add mongodbpnpm add mongodbYou will also need a MongoDB instance to connect to.Usage\\u200bEach chat history session stored in MongoDB must have a unique session id.import { MongoClient, ObjectId } from \"mongodb\";import { BufferMemory } from \"langchain/memory\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";import { MongoDBChatMessageHistory } from \"langchain/stores/message/mongodb\";const client = new MongoClient(process.env.MONGODB_ATLAS_URI || \"\");await client.connect();const collection = client.db(\"langchain\").collection(\"memory\");// generate a new sessionId stringconst sessionId = new ObjectId().toString();const memory = new BufferMemory({  chatHistory: new MongoDBChatMessageHistory({    collection,    sessionId,  }),});const model = new ChatOpenAI({  modelName: \"gpt-3.5-turbo\",  temperature: 0,});const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*  {    res1: {      text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"    }  }  */const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*  {    res1: {      text: \"You said your name was Jim.\"    }  }  */// See the chat history in the MongoDbconsole.log(await memory.chatHistory.getMessages());// clear chat historyawait memory.chatHistory.clear();API Reference:BufferMemory from langchain/memoryChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsMongoDBChatMessageHistory from langchain/stores/message/mongodbPreviousMomento-Backed Chat MemoryNextMotÃ¶rhead MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/mongodb', 'title': 'MongoDB Chat Memory | ð¦ï¸ð Langchain', 'description': 'For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a MongoDB instance.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nXata Chat Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toIntegrationsDynamoDB-Backed Chat MemoryFirestore Chat MemoryMomento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlanetScale Chat MemoryRedis-Backed Chat MemoryUpstash Redis-Backed Chat MemoryXata Chat MemoryZep MemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryIntegrationsXata Chat MemoryOn this pageXata Chat MemoryXata is a serverless data platform, based on PostgreSQL. It provides a type-safe TypeScript/JavaScript SDK for interacting with your database, and a\\nUI for managing your data.With the XataChatMessageHistory class, you can use Xata databases for longer-term persistence of chat sessions.Because Xata works via a REST API and has a pure TypeScript SDK, you can use this with Vercel Edge, Cloudflare Workers and any other Serverless environment.Setup\\u200bInstall the Xata CLI\\u200bnpm install @xata.io/cli -gCreate a database to be used as a vector store\\u200bIn the Xata UI create a new database. You can name it whatever you want, but for this example we\\'ll use langchain.When executed for the first time, the Xata LangChain integration will create the table used for storing the chat messages. If a table with that name already exists, it will be left untouched.Initialize the project\\u200bIn your project, run:xata initand then choose the database you created above. This will also generate a xata.ts or xata.js file that defines the client you can use to interact with the database. See the Xata getting started docs for more details on using the Xata JavaScript/TypeScript SDK.Usage\\u200bEach chat history session stored in Xata database must have a unique id.In this example, the getXataClient() function is used to create a new Xata client based on the environment variables. However, we recommend using the code generated by the xata init command, in which case you only need to import the getXataClient() function from the generated xata.ts file.import { BufferMemory } from \"langchain/memory\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";import { XataChatMessageHistory } from \"langchain/stores/message/xata\";import { BaseClient } from \"@xata.io/client\";// if you use the generated client, you don\\'t need this function.// Just import getXataClient from the generated xata.ts instead.const getXataClient = () => {  if (!process.env.XATA_API_KEY) {    throw new Error(\"XATA_API_KEY not set\");  }  if (!process.env.XATA_DB_URL) {    throw new Error(\"XATA_DB_URL not set\");  }  const xata = new BaseClient({    databaseURL: process.env.XATA_DB_URL,    apiKey: process.env.XATA_API_KEY,    branch: process.env.XATA_BRANCH || \"main\",  });  return xata;};const memory = new BufferMemory({  chatHistory: new XataChatMessageHistory({    table: \"messages\",    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation    client: getXataClient(),    apiKey: process.env.XATA_API_KEY, // The API key is needed for creating the table.  }),});const model = new ChatOpenAI();const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsXataChatMessageHistory from langchain/stores/message/xataWith pre-created table\\u200bIf you don\\'t want the code to always check if the table exists, you can create the table manually in the Xata UI and pass createTable: false to the constructor. The table must have the following columns:sessionId of type Stringtype of type Stringrole of type Stringcontent of type Textname of type StringadditionalKwargs of type Textimport { BufferMemory } from \"langchain/memory\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import { ConversationChain } from \"langchain/chains\";import { XataChatMessageHistory } from \"langchain/stores/message/xata\";import { BaseClient } from \"@xata.io/client\";// Before running this example, see the docs at// https://js.langchain.com/docs/modules/memory/integrations/xata// if you use the generated client, you don\\'t need this function.// Just import getXataClient from the generated xata.ts instead.const getXataClient = () => {  if (!process.env.XATA_API_KEY) {    throw new Error(\"XATA_API_KEY not set\");  }  if (!process.env.XATA_DB_URL) {    throw new Error(\"XATA_DB_URL not set\");  }  const xata = new BaseClient({    databaseURL: process.env.XATA_DB_URL,    apiKey: process.env.XATA_API_KEY,    branch: process.env.XATA_BRANCH || \"main\",  });  return xata;};const memory = new BufferMemory({  chatHistory: new XataChatMessageHistory({    table: \"messages\",    sessionId: new Date().toISOString(), // Or some other unique identifier for the conversation    client: getXataClient(),    createTable: false, // Explicitly set to false if the table is already created  }),});const model = new ChatOpenAI();const chain = new ConversationChain({ llm: model, memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });/*{  res1: {    text: \"Hello Jim! It\\'s nice to meet you. My name is AI. How may I assist you today?\"  }}*/const res2 = await chain.call({ input: \"What did I just say my name was?\" });console.log({ res2 });/*{  res1: {    text: \"You said your name was Jim.\"  }}*/API Reference:BufferMemory from langchain/memoryChatOpenAI from langchain/chat_models/openaiConversationChain from langchain/chainsXataChatMessageHistory from langchain/stores/message/xataPreviousUpstash Redis-Backed Chat MemoryNextZep MemorySetupInstall the Xata CLICreate a database to be used as a vector storeInitialize the projectUsageWith pre-created tableCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/integrations/xata', 'title': 'Xata Chat Memory | ð¦ï¸ð Langchain', 'description': 'Xata is a serverless data platform, based on PostgreSQL. It provides a type-safe TypeScript/JavaScript SDK for interacting with your database, and a', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nConversation buffer memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toConversation buffer memoryUsing Buffer Memory with Chat ModelsConversation buffer window memoryBuffer Window MemoryEntity memoryHow to use multiple memory classes in the same chainConversation summary memoryConversation summary buffer memoryVector store-backed memoryIntegrationsAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryHow-toConversation buffer memoryConversation buffer memoryThis notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.We can first extract it as a string.import { OpenAI } from \"langchain/llms/openai\";import { BufferMemory } from \"langchain/memory\";import { ConversationChain } from \"langchain/chains\";const model = new OpenAI({});const memory = new BufferMemory();const chain = new ConversationChain({ llm: model, memory: memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });{response: \" Hi Jim! It\\'s nice to meet you. My name is AI. What would you like to talk about?\"}const res2 = await chain.call({ input: \"What\\'s my name?\" });console.log({ res2 });{response: \\' You said your name is Jim. Is there anything else you would like to talk about?\\'}You can also load messages into a BufferMemory instance by creating and passing in a ChatHistory object.\\nThis lets you easily pick up state from past conversations:import { BufferMemory, ChatMessageHistory } from \"langchain/memory\";import { HumanMessage, AIMessage } from \"langchain/schema\";const pastMessages = [  new HumanMessage(\"My name\\'s Jonas\"),  new AIMessage(\"Nice to meet you, Jonas!\"),];const memory = new BufferMemory({  chatHistory: new ChatMessageHistory(pastMessages),});PreviousMemoryNextUsing Buffer Memory with Chat ModelsCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/how_to/buffer', 'title': 'Conversation buffer memory | ð¦ï¸ð Langchain', 'description': 'This notebook shows how to use BufferMemory. This memory allows for storing of messages, then later formats the messages into a prompt input variable.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nConversation buffer window memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toConversation buffer memoryUsing Buffer Memory with Chat ModelsConversation buffer window memoryBuffer Window MemoryEntity memoryHow to use multiple memory classes in the same chainConversation summary memoryConversation summary buffer memoryVector store-backed memoryIntegrationsAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryHow-toConversation buffer window memoryConversation buffer window memoryConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too largeLet\\'s first explore the basic functionality of this type of memory.import { OpenAI } from \"langchain/llms/openai\";import { BufferWindowMemory } from \"langchain/memory\";import { ConversationChain } from \"langchain/chains\";const model = new OpenAI({});const memory = new BufferWindowMemory({ k: 1 });const chain = new ConversationChain({ llm: model, memory: memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });{response: \" Hi Jim! It\\'s nice to meet you. My name is AI. What would you like to talk about?\"}const res2 = await chain.call({ input: \"What\\'s my name?\" });console.log({ res2 });{response: \\' You said your name is Jim. Is there anything else you would like to talk about?\\'}PreviousUsing Buffer Memory with Chat ModelsNextBuffer Window MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/how_to/buffer_window', 'title': 'Conversation buffer window memory | ð¦ï¸ð Langchain', 'description': 'ConversationBufferWindowMemory keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nBuffer Window Memory | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toConversation buffer memoryUsing Buffer Memory with Chat ModelsConversation buffer window memoryBuffer Window MemoryEntity memoryHow to use multiple memory classes in the same chainConversation summary memoryConversation summary buffer memoryVector store-backed memoryIntegrationsAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryHow-toBuffer Window MemoryBuffer Window MemoryBufferWindowMemory keeps track of the back-and-forths in conversation, and then uses a window of size k to surface the last k back-and-forths to use as memory.import { OpenAI } from \"langchain/llms/openai\";import { BufferWindowMemory } from \"langchain/memory\";import { ConversationChain } from \"langchain/chains\";const model = new OpenAI({});const memory = new BufferWindowMemory({ k: 1 });const chain = new ConversationChain({ llm: model, memory: memory });const res1 = await chain.call({ input: \"Hi! I\\'m Jim.\" });console.log({ res1 });{response: \" Hi Jim! It\\'s nice to meet you. My name is AI. What would you like to talk about?\"}const res2 = await chain.call({ input: \"What\\'s my name?\" });console.log({ res2 });{response: \\' You said your name is Jim. Is there anything else you would like to talk about?\\'}PreviousConversation buffer window memoryNextEntity memoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/how_to/buffer_window_memory', 'title': 'Buffer Window Memory | ð¦ï¸ð Langchain', 'description': 'BufferWindowMemory keeps track of the back-and-forths in conversation, and then uses a window of size k to surface the last k back-and-forths to use as memory.', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nUsing Buffer Memory with Chat Models | ð¦ï¸ð Langchain\\n\\n\\n\\n\\n\\nSkip to main contentð¦ï¸ð LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/\\u200bORetrievalChainsMemoryHow-toConversation buffer memoryUsing Buffer Memory with Chat ModelsConversation buffer window memoryBuffer Window MemoryEntity memoryHow to use multiple memory classes in the same chainConversation summary memoryConversation summary buffer memoryVector store-backed memoryIntegrationsAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryHow-toUsing Buffer Memory with Chat ModelsUsing Buffer Memory with Chat ModelsThis example covers how to use chat-specific memory classes with chat models.\\nThe key thing to notice is that setting returnMessages: true makes the memory return a list of chat messages instead of a string.import { ConversationChain } from \"langchain/chains\";import { ChatOpenAI } from \"langchain/chat_models/openai\";import {  ChatPromptTemplate,  HumanMessagePromptTemplate,  SystemMessagePromptTemplate,  MessagesPlaceholder,} from \"langchain/prompts\";import { BufferMemory } from \"langchain/memory\";export const run = async () => {  const chat = new ChatOpenAI({ temperature: 0 });  const chatPrompt = ChatPromptTemplate.fromPromptMessages([    SystemMessagePromptTemplate.fromTemplate(      \"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"    ),    new MessagesPlaceholder(\"history\"),    HumanMessagePromptTemplate.fromTemplate(\"{input}\"),  ]);  const chain = new ConversationChain({    memory: new BufferMemory({ returnMessages: true, memoryKey: \"history\" }),    prompt: chatPrompt,    llm: chat,  });  const response = await chain.call({    input: \"hi! whats up?\",  });  console.log(response);};API Reference:ConversationChain from langchain/chainsChatOpenAI from langchain/chat_models/openaiChatPromptTemplate from langchain/promptsHumanMessagePromptTemplate from langchain/promptsSystemMessagePromptTemplate from langchain/promptsMessagesPlaceholder from langchain/promptsBufferMemory from langchain/memoryPreviousConversation buffer memoryNextConversation buffer window memoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\\n\\n\\n\\n', metadata={'source': 'https://js.langchain.com/docs/modules/memory/how_to/buffer_memory_chat', 'title': 'Using Buffer Memory with Chat Models | ð¦ï¸ð Langchain', 'description': 'This example covers how to use chat-specific memory classes with chat models.', 'language': 'en'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "#from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "from app import get_secrets\n",
    "os.environ['OPENAI_API_KEY'] = get_secrets(llm='OPENAI')\n",
    "embedding_function =OpenAIEmbeddings()\n",
    "\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# split it into chunks\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=20)\n",
    "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "#docs_split=docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs_split, embedding_function,persist_directory=\"./chroma_db\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.chroma.Chroma at 0x137bfa290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query it\n",
    "query = \"What is langchain memory\"\n",
    "docs_q = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "for d in docs_q:\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "params = {\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "credentials = {\n",
    "    'url': \"https://us-south.ml.cloud.ibm.com\",\n",
    "    'apikey' : \n",
    "}\n",
    "\n",
    "project_id = \n",
    "\n",
    "flan_t5_model = Model(\n",
    "model_id=\"google/flan-t5-xxl\",\n",
    "credentials=credentials,\n",
    "project_id=project_id)\n",
    "llm = WatsonxLLM(model=flan_t5_model)\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "#memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "#retriever = db.as_retriever()\n",
    "# Retrieve more documents with higher diversity- useful if your dataset has many similar documents\n",
    "retriever=db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 10, 'lambda_mult': 0.25})\n",
    "\n",
    "# Fetch more documents for the MMR algorithm to consider, but only return the top 5\n",
    "#retriever=db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 50})\n",
    "\n",
    "# Only retrieve documents that have a relevance score above a certain threshold\n",
    "#retriever= db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.7})\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "#retriever=db.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "# Use a filter to only retrieve documents from a specific paper \n",
    "#docsearch.as_retriever(search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}})\n",
    "\n",
    "\n",
    "\n",
    "qa_s = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,chain_type_kwargs=chain_type_kwargs)\n",
    "#chain = LLMChain(llm=llm, prompt=prompt, verbose=True, output_key='output', memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'memory allows for storing of messages, then later formats the messages into a prompt input variable.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"what is memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Momento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlane'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"what types of memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are many types of memory.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"how many types of memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Momento-Backed Chat MemoryMongoDB Chat MemoryMotÃ¶rhead MemoryPlane'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('What are the types of memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "#from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "#from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "docs_split = text_splitter.split_documents(docs)\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs_split, embedding_function,persist_directory=\"./chroma_db\")\n",
    "db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Redis-Backed Chat Memory'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s.run(\"what is memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 50 is greater than number of elements in index 30, updating n_results = 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'BufferMemory'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s.run(\"what types of memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 50 is greater than number of elements in index 30, updating n_results = 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are two types of memory.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s.run(\"how many types of memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 50 is greater than number of elements in index 30, updating n_results = 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Buffer Window Memory |  Langchain Momento-Backed Chat Memory |  Lang'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_s.run('What are the types of memory')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
