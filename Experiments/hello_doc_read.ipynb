{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/Users/deanchanter/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e02706953f45ccb0de680851cc15ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "dataset_name = \"wikipedia\"\n",
    "page_content_column = \"text\"\n",
    "name = \"20220301.simple\"\n",
    "\n",
    "\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column, name)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  langchain.schema import Document\n",
    "import json\n",
    "from typing import Iterable\n",
    "\n",
    "def save_docs_to_jsonl(array:Iterable[Document], file_path:str)->None:\n",
    "    with open(file_path, 'w') as jsonl_file:\n",
    "        for doc in array:\n",
    "            jsonl_file.write(doc.json() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_docs_to_jsonl(docs,'wiki.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/db/mixins/embeddings_queue.py\", line 263, in _notify_one\n",
      "    sub.callback([embedding])\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/segment/impl/metadata/sqlite.py\", line 393, in _write_metadata\n",
      "    self._insert_record(cur, record, True)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/segment/impl/metadata/sqlite.py\", line 227, in _insert_record\n",
      "    self._update_metadata(cur, id, record[\"metadata\"])\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/segment/impl/metadata/sqlite.py\", line 253, in _update_metadata\n",
      "    cur.execute(sql, params)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/deanchanter/miniconda3/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
      "    msg = self.format(record)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deanchanter/miniconda3/lib/python3.11/logging/__init__.py\", line 953, in format\n",
      "    return fmt.format(record)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deanchanter/miniconda3/lib/python3.11/logging/__init__.py\", line 687, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/deanchanter/miniconda3/lib/python3.11/logging/__init__.py\", line 377, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/deanchanter/miniconda3/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/deanchanter/miniconda3/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/deanchanter/miniconda3/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/n6/vtq0fgnd0476llkgw23cpwlc0000gn/T/ipykernel_69606/2687468036.py\", line 37, in <module>\n",
      "    db = Chroma.from_documents(docs_split, embedding_function,persist_directory=\"./chroma_db_wiki\")\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/langchain/vectorstores/chroma.py\", line 612, in from_documents\n",
      "    return cls.from_texts(\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/langchain/vectorstores/chroma.py\", line 576, in from_texts\n",
      "    chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/langchain/vectorstores/chroma.py\", line 208, in add_texts\n",
      "    self._collection.upsert(\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/api/models/Collection.py\", line 298, in upsert\n",
      "    self._client._upsert(\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/api/segment.py\", line 284, in _upsert\n",
      "    self._producer.submit_embedding(coll[\"topic\"], r)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/db/mixins/embeddings_queue.py\", line 142, in submit_embedding\n",
      "    self._notify_all(topic_name, embedding_record)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/db/mixins/embeddings_queue.py\", line 249, in _notify_all\n",
      "    self._notify_one(sub, embedding)\n",
      "  File \"/Users/deanchanter/Documents/GitHub/comma-chameleons/env/lib/python3.11/site-packages/chromadb/db/mixins/embeddings_queue.py\", line 266, in _notify_one\n",
      "    logger.error(\n",
      "Message: 'Exception occurred invoking consumer for subscription aa3bf8a0-9c82-47d4-a3c1-02aee7849620to topic persistent://default/default/a8f1431b-2c93-4671-b55e-9c18c5b9f93d for embedding id 24857bfa-54d7-11ee-9abe-acde48001122 '\n",
      "Arguments: (KeyboardInterrupt(),)\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "#from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "#from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "#from langchain.document_loaders import TextLoader\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "#from app import get_secrets\n",
    "#os.environ['OPENAI_API_KEY'] = get_secrets(llm='OPENAI')\n",
    "#embedding_function =OpenAIEmbeddings()\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-large-en\")\n",
    "\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "#model_kwargs = {'device': 'cuda'}\n",
    "#encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "bge_hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    " #   model_kwargs=model_kwargs,\n",
    " #   encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = TokenTextSplitter.from_huggingface_tokenizer(tokenizer,chunk_size=200, chunk_overlap=10)\n",
    "#text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=25)\n",
    "docs_split = text_splitter.transform_documents(docs)\n",
    "#docs_split=docs\n",
    "#print(docs_split[0])\n",
    "embedding_function= bge_hf\n",
    "\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs_split, embedding_function,persist_directory=\"./chroma_db_wiki\")\n",
    "db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load db\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "#model_kwargs = {'device': 'cuda'}\n",
    "#encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "bge_hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    ")\n",
    "db = Chroma(embedding_function = bge_hf,persist_directory=\"./chroma_db\")\n",
    "db.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': True}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': True, 'pooling_mode_mean_tokens': False, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "), model_name='BAAI/bge-small-en', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_instruction='Represent this question for searching relevant passages: ')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#Y132sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# query it\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#Y132sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWho is Allen Turing\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#Y132sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m docs_q \u001b[39m=\u001b[39m db\u001b[39m.\u001b[39msimilarity_search(\u001b[39m'\u001b[39m\u001b[39mwhat is memory\u001b[39m\u001b[39m'\u001b[39m,search_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmmr\u001b[39m\u001b[39m\"\u001b[39m, search_kwargs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mk\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfetch_k\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m50\u001b[39m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#Y132sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# print results\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#Y132sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m docs_q:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db' is not defined"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "query = \"Who is Allen Turing\"\n",
    "docs_q = db.similarity_search('what is memory',search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 50})\n",
    "\n",
    "# print results\n",
    "for d in docs_q:\n",
    "    print(d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "#from langchain.prompts.pipeline import PipelinePromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain, ConversationalRetrievalChain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "params = {\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.TEMPERATURE: 0.8,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: .95\n",
    "}\n",
    "credentials = {\n",
    "    'url': \"https://us-south.ml.cloud.ibm.com\",\n",
    "    'apikey' : '5Oz7Ma1ZCctfI0I9FU1kbZC6YRWm9jRKddtL-8mJYREx'\n",
    "}\n",
    "\n",
    "project_id = '0ec8f431-3b27-41a3-a540-98d7f62ca1ad'\n",
    "\n",
    "\n",
    "llama = ModelTypes.LLAMA_2_70B_CHAT\n",
    "\n",
    "llama_model = Model(\n",
    "model_id=llama,\n",
    "params=params,\n",
    "credentials=credentials,\n",
    "project_id=project_id)\n",
    "llm = WatsonxLLM(model=llama_model)\n",
    "\n",
    "\n",
    "prompt_template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate.from(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "#memory = ConversationBufferMemory(memory_key='chat_history')\n",
    "#retriever = db.as_retriever()\n",
    "# Retrieve more documents with higher diversity- useful if your dataset has many similar documents\n",
    "#retriever=db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 8, 'lambda_mult': 0.25})\n",
    "\n",
    "# Fetch more documents for the MMR algorithm to consider, but only return the top 5\n",
    "retriever=db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 50})\n",
    "\n",
    "# Only retrieve documents that have a relevance score above a certain threshold\n",
    "#retriever= db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={'score_threshold': 0.8})\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "#retriever=db.as_retriever(search_kwargs={'k': 5})\n",
    "\n",
    "# Use a filter to only retrieve documents from a specific paper \n",
    "#docsearch.as_retriever(search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}})\n",
    "memory = ConversationSummaryMemory(llm=llm,memory_key=\"chat_history\",return_messages=True)\n",
    "\n",
    "conv = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory, prompt = PROMPT)\n",
    "\n",
    "#qa_s = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever,chain_type_kwargs=chain_type_kwargs)\n",
    "#chain = LLMChain(llm=llm, prompt=prompt, verbose=True, output_key='output', memory=memory)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing docs\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "* \"Chains\"\n",
      "* \"Chain\"\n",
      "* \"chaining LLMs\"\n",
      "* \"LangChain provides the Chain interface\"\n",
      "* \"a Chain very generically as a sequence of calls to components, which can include other chains\"\n",
      "* \"BaseChain\"\n",
      "* \"ChainValues\"\n",
      "* \"abstract class BaseChain\"\n",
      "* \"memory\"\n",
      "* \"call\"\n",
      "* \"runManager\"\n",
      "* \"input keys\"\n",
      "* \"output keys\"\n",
      "\n",
      "Note: The extracted parts are not edited, they are copied as is from the context.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "* Debugging chains\n",
      "* Adding memory (state)\n",
      "* Chain object\n",
      "* LangChain Expression Language (LCEL)\n",
      "* Modules\n",
      "* Memory object\n",
      "* Stateful\n",
      "\n",
      "Please note that the extracted parts are not necessarily the only relevant parts, and the question may require additional information from the context to be fully answered.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "* Debugging chains\n",
      "* Chainobject\n",
      "* Settingverbosetotrue\n",
      "* ConversationChain\n",
      "* LLM output post-processing\n",
      "* BufferMemory\n",
      "* Prompt\n",
      "* Input\n",
      "* History\n",
      "* LLM run\n",
      "* HumanMessage\n",
      "\n",
      "Please note that the extracted parts are not necessarily the only relevant parts, but rather the parts that are most directly related to the question.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "* Chain\n",
      "* Langchain\n",
      "* ChainValues\n",
      "* LangChainDocs\n",
      "* Use cases\n",
      "* API\n",
      "* Modules\n",
      "* Model I/O\n",
      "* Retrieval\n",
      "* Chains\n",
      "* Memory\n",
      "* Agents\n",
      "* Callbacks\n",
      "* LangChain Expression Language\n",
      "* Cookbook\n",
      "* Ecosystem\n",
      "* Community navigator\n",
      "* Additional resources\n",
      "\n",
      "Please note that the extracted parts are not necessarily the only relevant parts, but rather the parts that are most directly related to the question.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "* \"Sequential chains allow you to connect multiple chains and compose them into pipelines that execute some specific scenario.\"\n",
      "* \"There are two types of sequential chains: SimpleSequentialChain and SequentialChain.\"\n",
      "* \"A SimpleSequentialChain is a chain that allows you to join multiple single-input/single-output chains into one chain.\"\n",
      "* \"An example of a SimpleSequentialChain is a chain that generates a synopsis of a play and then generates a review of the play based on the synopsis.\"\n",
      "\n",
      "NO_OUTPUT\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "* LangChain Expression Language (LCEL)\n",
      "* LangChain Expression Language or LCEL is a declarative way to easily compose chains together.\n",
      "\n",
      "Please provide the actual answer to the question based on the extracted context.\n",
      "\n",
      "If the extracted context does not contain enough information to answer the question, return NO_OUTPUT.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "* SerializedSequentialChain\n",
      "* LangchainSkip to main content\n",
      "* LangChainDocs\n",
      "* Use cases\n",
      "* API\n",
      "* LangSmith\n",
      "* Python Docs\n",
      "* CTRLK\n",
      "* Get started\n",
      "* Introduction\n",
      "* Installation\n",
      "* Quickstart\n",
      "* Modules\n",
      "* Model I/O\n",
      "* Retrieval\n",
      "* Chains\n",
      "* Memory\n",
      "* Agents\n",
      "* Callbacks\n",
      "* Modules\n",
      "* LangChain Expression Language\n",
      "* Cookbook\n",
      "* LangChain Expression Language (LCEL)\n",
      "* Interface\n",
      "* Guides\n",
      "* Ecosystem\n",
      "* Additional resources\n",
      "* Community navigator\n",
      "\n",
      "Please note that the extracted parts are not necessarily the only relevant parts, but rather the parts that can be directly related to the question.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "LangChain, Inc.\n",
      "\n",
      "Please provide your answer.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "#LLM Compressor....too slow\n",
    "\"\"\"\n",
    "retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 8, 'lambda_mult': 0.25})\n",
    "llm = llm_llama_temp_0p1\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents('What is a chain in langchain?')\n",
    "pretty_print_docs(compressed_docs)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "#from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "\n",
    "# Helper function for printing docs\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "\n",
    "\n",
    "params_temp_0p1 = {\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.TEMPERATURE: 0.1,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: .95\n",
    "}\n",
    "credentials = {\n",
    "    'url': \"https://us-south.ml.cloud.ibm.com\",\n",
    "    'apikey' : '5Oz7Ma1ZCctfI0I9FU1kbZC6YRWm9jRKddtL-8mJYREx'\n",
    "}\n",
    "\n",
    "project_id = '0ec8f431-3b27-41a3-a540-98d7f62ca1ad'\n",
    "\n",
    "\n",
    "llama = ModelTypes.LLAMA_2_70B_CHAT\n",
    "\n",
    "llama_model_temp_0p1 = Model(\n",
    "model_id=llama,\n",
    "params=params_temp_0p1,\n",
    "credentials=credentials,\n",
    "project_id=project_id)\n",
    "llm_llama_temp_0p1 = WatsonxLLM(model=llama_model_temp_0p1)\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate three\n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions seperated by newlines. The default context of the topic is langchain.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "base_retriever = db.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "#embeddings = bge_hf\n",
    "embeddings = db.embeddings\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.85)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=embeddings_filter, base_retriever=base_retriever)\n",
    "\n",
    "#too slow\n",
    "#compressor = LLMChainExtractor.from_llm(llm_llama_temp_0p1)\n",
    "#compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base_retriever)\n",
    "\n",
    "\n",
    "retriever_llm_chain = LLMChain(llm=llm_llama_temp_0p1, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "multi_retriever = MultiQueryRetriever(\n",
    "    retriever=compression_retriever, llm_chain=retriever_llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['Version 1: What is the Turing Test?', '    Version 2: Who is the father of computer science?', '    Version 3: What is the history of artificial intelligence?', '    Please provide your answer in plain text.', '', '    -----------------------------------------------------------------------------------------', '    Question: Who is Allen Turing?', '    -----------------------------------------------------------------------------------------', '    Version 1:', '    Who is the mathematician who developed the Turing Machine?', '', '    Version 2:', '    Who is the computer scientist who proposed the Turing Test?', '', '    Version 3:', '    Who is the British logician who was convicted of gross indecency for his homosexuality?', '', '    -----------------------------------------------------------------------------------------', '    Please provide your answer in plain text.', '', '    Note: The default context of the topic is langchain.', '    -----------------------------------------------------------------------------------------', '    Answer:', '    -----------------------------------------------------------------------------------------', '    Version 1:', '    The mathematician who developed the Turing Machine is Alan Turing.', '', '    Version 2:', '    The computer scientist who proposed the Turing Test is Alan Turing.', '', '    Version 3:', '    The British logician who was convicted of gross indecency for his homosexuality is Alan Turing.', '', '    -----------------------------------------------------------------------------------------', '    Please let me know if you have any questions or need further clarification.', '', '    -----------------------------------------------------------------------------------------', '    Thank you for your response. Here are three alternative questions that can help retrieve relevant documents from a vector database:', '', '    Version 1: Who developed the theoretical framework for modern computing?', '', '    Version 2: Who is known for their work on the theoretical foundations of computation?', '', '    Version 3: Who made significant contributions to the development of artificial intelligence?', '', '    -----------------------------------------------------------------------------------------', '    I hope this helps! Let me know if you have any other questions.', '', '    -----------------------------------------------------------------------------------------', '', '    Thank you for your response. Here are three alternative questions that can help retrieve relevant documents from a vector database:', '', '    Version 1: Who is known for their work on the theoretical foundations of computation?', '', '    Version 2: Who developed the theoretical framework for modern computing?', '', '    Version 3: Who made significant contributions to the development of artificial intelligence?', '', '    -----------------------------------------------------------------------------------------', '    I hope this helps! Let me know if you have any other questions.', '', '    -----------------------------------------------------------------------------------------', '', '    Thank you for your response. Here are three alternative questions that can help retrieve relevant documents from a vector database:', '', '    Version 1: Who made significant contributions to the development of artificial intelligence?', '', '    Version 2: Who is known for their work on the theoretical foundations of computation?', '', '    Version 3: Who developed the theoretical framework for modern computing?', '', '    -----------------------------------------------------------------------------------------', '    I hope this helps! Let me know if you have any other questions.', '', '    -----------------------------------------------------------------------------------------', '', '    Thank you for your response. Here are three alternative questions that can help retrieve relevant documents from a vector database:', '', '    Version 1: Who is known for their work on the theoretical foundations of computation?', '', '    Version 2: Who developed the theoretical framework for modern computing?', '', '    Version 3: Who made significant contributions to the development of artificial intelligence?', '', '    -----------------------------------------------------------------------------------------', '    I hope this helps! Let me know if you have any other questions.', '', '    -----------------------------------------------------------------------------------------', '', '    Thank you for your response. Here are three alternative questions that can help retrieve relevant documents from a vector database:', '', '    Version 1: Who developed the theoretical framework for modern computing?', '', '    Version 2: Who is known for their work on the theoretical foundations of computation?', '', '    Version 3: Who made significant contributions to the development of artificial intelligence?', '', '    -----------------------------------------------------------------------------------------', '    I hope this helps! Let me know if you have any other']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "# Set logging for the queries\n",
    "\n",
    "# Results\n",
    "unique_docs = multi_retriever.get_relevant_documents(\n",
    "    query='Who is Allen Turing?'\n",
    ")\n",
    "len(unique_docs)\n",
    "\n",
    "# Other inputs\n",
    "#question = \"What are the approaches to Task Decomposition?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "angchain/â€‹vectorstores/â€‹prismalangchain/â€‹vectorstores/â€‹qdrantlangchain/â€‹vectorstores/â€‹redislangchain/â€‹vectorstores/â€‹singlestorelangchain/â€‹vectorstores/â€‹supabaselangchain/â€‹vectorstores/â€‹tigrislangchain/â€‹vectorstores/â€‹typeormlangchain/â€‹vectorstores/â€‹typesenselangchain/â€‹vectorstores/â€‹usearchlangchain/â€‹vectorstores/â€‹vectaralangchain/â€‹vectorstores/â€‹voylangchain/â€‹vectorstores/â€‹weaviatelangchain/â€‹vectorstores/â€‹xatalangchain/â€‹vectorstores/â€‹zepAPI referencelangchain/â€‹experimental/â€‹chat_models/â€‹anthropic_functionsClassesAnthropicFunctionsOn this pageAnthropicFunctionsWrapper around Anthropic large language models.To use you should have the@anthropic-ai/sdkpackage installed, with theANTHROPIC_API_KEYenvironment variable set.Remarksâ€‹Any parameters that are valid to be passed toanthropic.completecan be passed throughinvocationKwargs,\n",
      "even if not explicitly available on this class.Hierarchyâ€‹ChatAnthropic<ChatAnthropicFunctionsCallOptions>.AnthropicFunctionsConstructorsâ€‹constructor()â€‹new AnthropicFunctions(fields?:Partial<AnthropicInput>&BaseLanguageModelParams):AnthropicFunctionsParametersâ€‹ParameterTypefields?Partial<AnthropicInput>&BaseLanguageModelParamsReturnsâ€‹AnthropicFunctionsOverridesâ€‹ChatAnthropic.constructorDefined inâ€‹langchain/src/experimental/chat_models/anthropic_functions.ts:52Propertiesâ€‹CallOptionsâ€‹CallOptions:ChatAnthropicFunctionsCallOptionsInherited fromâ€‹ChatAnthropic.CallOptionsDefined inâ€‹langchain/src/base_language/index.ts:126langchain/\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "elangchain/â€‹vectorstores/â€‹prismalangchain/â€‹vectorstores/â€‹qdrantlangchain/â€‹vectorstores/â€‹redislangchain/â€‹vectorstores/â€‹singlestorelangchain/â€‹vectorstores/â€‹supabaselangchain/â€‹vectorstores/â€‹tigrislangchain/â€‹vectorstores/â€‹typeormlangchain/â€‹vectorstores/â€‹typesenselangchain/â€‹vectorstores/â€‹usearchlangchain/â€‹vectorstores/â€‹vectaralangchain/â€‹vectorstores/â€‹voylangchain/â€‹vectorstores/â€‹weaviatelangchain/â€‹vectorstores/â€‹xatalangchain/â€‹vectorstores/â€‹zepAPI referencelangchain/â€‹chains/â€‹openai_functionsOn this pagelangchain/chains/openai_functionsIndexâ€‹Type Aliasesâ€‹StructuredOutputChainInputFunctionsâ€‹createStructuredOutputChaincreateStructuredOutputChainFromZodReferencesâ€‹OpenAPIChainOptionsâ€‹Re-exportsOpenAPIChainOptionsDefined inâ€‹langchain/src/chains/openai_functions/index.ts:10TaggingChainOptionsâ€‹Re-exportsTaggingChainOptionsDefined inâ€‹langchain/src/chains/openai_functions/index.ts:6createExtractionChainâ€‹Re-exportscreateExtractionChainDefined inâ€‹langchain/src/chains/openai_functions/index.ts:2createExtractionChainFromZodâ€‹Re-exportscreateExtractionChainFromZodDefined inâ€‹langchain/src/chains/openai_functions/index.ts:3createOpenAPIChainâ€‹Re-exportscreateOpenAPIChainDefined inâ€‹langchain/src/chains/openai_functions/index.ts:10createTaggingChainâ€‹Re-exportscreateTaggingChainDefined inâ€‹langchain/src/chains/openai_functions/index.ts:7createTaggingChain\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "ings/â€‹googlevertexailangchain/â€‹experimental/â€‹plan_and_executelangchain/â€‹hublangchainlangchain/â€‹llms/â€‹ai21langchain/â€‹llms/â€‹aleph_alphalangchain/â€‹llms/â€‹baselangchain/â€‹llms/â€‹bedrocklangchain/â€‹llms/â€‹coherelangchain/â€‹llms/â€‹googlepalmlangchain/â€‹llms/â€‹googlevertexailangchain/â€‹llms/â€‹hflangchain/â€‹llms/â€‹llama_cpplangchain/â€‹llms/â€‹loadlangchain/â€‹llms/â€‹ollamalangchain/â€‹llms/â€‹openailangchain/â€‹llms/â€‹raycastlangchain/â€‹llms/â€‹replicatelangchain/â€‹llms/â€‹sagemaker_endpointlangchain/â€‹llms/â€‹writerlangchain/â€‹loadlangchain/â€‹load/â€‹serializablelangchain/â€‹memorylangchain/â€‹memory/â€‹zeplangchain/â€‹output_parserslangchain/â€‹output_parsers/â€‹expressionlangchain/â€‹promptslangchain/â€‹prompts/â€‹loadlangchain/â€‹retrievers/â€‹amazon_kendralangchain/â€‹retrievers/â€‹contextual_compressionlangchain/â€‹retrievers/â€‹databerrylangchain/â€‹retrievers/â€‹document_compressorslangchain/â€‹retrievers/â€‹document_compressors/â€‹chain_extractlangchain/â€‹retrievers/â€‹hydelangchain/â€‹retrievers/â€‹metallangchain/â€‹retrievers/â€‹multi_vectorlangchain/â€‹retrievers/â€‹parent_documentlangchain/â€‹retrievers/â€‹remotelangchain/â€‹retrievers/â€‹score_thresholdlangchain/â€‹retrievers/â€‹self_querylangchain/â€‹retrievers/â€‹self_query/â€‹chromalangchain/â€‹retrievers/â€‹self_query/â€‹functionallangchain/â€‹retrievers/â€‹self_query/â€‹pineconelangchain/â€‹retrievers/â€‹self_query\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "ings/â€‹googlevertexailangchain/â€‹experimental/â€‹plan_and_executelangchain/â€‹hublangchainlangchain/â€‹llms/â€‹ai21langchain/â€‹llms/â€‹aleph_alphalangchain/â€‹llms/â€‹baselangchain/â€‹llms/â€‹bedrocklangchain/â€‹llms/â€‹coherelangchain/â€‹llms/â€‹googlepalmlangchain/â€‹llms/â€‹googlevertexailangchain/â€‹llms/â€‹hflangchain/â€‹llms/â€‹llama_cpplangchain/â€‹llms/â€‹loadlangchain/â€‹llms/â€‹ollamalangchain/â€‹llms/â€‹openailangchain/â€‹llms/â€‹raycastlangchain/â€‹llms/â€‹replicatelangchain/â€‹llms/â€‹sagemaker_endpointlangchain/â€‹llms/â€‹writerlangchain/â€‹loadlangchain/â€‹load/â€‹serializablelangchain/â€‹memorylangchain/â€‹memory/â€‹zeplangchain/â€‹output_parserslangchain/â€‹output_parsers/â€‹expressionlangchain/â€‹promptslangchain/â€‹prompts/â€‹loadlangchain/â€‹retrievers/â€‹amazon_kendralangchain/â€‹retrievers/â€‹contextual_compressionlangchain/â€‹retrievers/â€‹databerrylangchain/â€‹retrievers/â€‹document_compressorslangchain/â€‹retrievers/â€‹document_compressors/â€‹chain_extractlangchain/â€‹retrievers/â€‹hydelangchain/â€‹retrievers/â€‹metallangchain/â€‹retrievers/â€‹multi_vectorlangchain/â€‹retrievers/â€‹parent_documentlangchain/â€‹retrievers/â€‹remotelangchain/â€‹retrievers/â€‹score_thresholdlangchain/â€‹retrievers/â€‹self_querylangchain/â€‹retrievers/â€‹self_query/â€‹chromalangchain/â€‹retrievers/â€‹self_query/â€‹functionallangchain/â€‹retrievers/â€‹self_query/â€‹pineconelangchain/â€‹retrievers/â€‹self_query\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 5:\n",
      "\n",
      "/â€‹googlevertexailangchain/â€‹experimental/â€‹plan_and_executelangchain/â€‹hublangchainlangchain/â€‹llms/â€‹ai21langchain/â€‹llms/â€‹aleph_alphalangchain/â€‹llms/â€‹baselangchain/â€‹llms/â€‹bedrocklangchain/â€‹llms/â€‹coherelangchain/â€‹llms/â€‹googlepalmlangchain/â€‹llms/â€‹googlevertexailangchain/â€‹llms/â€‹hflangchain/â€‹llms/â€‹llama_cpplangchain/â€‹llms/â€‹loadlangchain/â€‹llms/â€‹ollamalangchain/â€‹llms/â€‹openailangchain/â€‹llms/â€‹raycastlangchain/â€‹llms/â€‹replicatelangchain/â€‹llms/â€‹sagemaker_endpointlangchain/â€‹llms/â€‹writerlangchain/â€‹loadlangchain/â€‹load/â€‹serializablelangchain/â€‹memorylangchain/â€‹memory/â€‹zeplangchain/â€‹output_parserslangchain/â€‹output_parsers/â€‹expressionlangchain/â€‹promptslangchain/â€‹prompts/â€‹loadlangchain/â€‹retrievers/â€‹amazon_kendralangchain/â€‹retrievers/â€‹contextual_compressionlangchain/â€‹retrievers/â€‹databerrylangchain/â€‹retrievers/â€‹document_compressorslangchain/â€‹retrievers/â€‹document_compressors/â€‹chain_extractlangchain/â€‹retrievers/â€‹hydelangchain/â€‹retrievers/â€‹metallangchain/â€‹retrievers/â€‹multi_vectorlangchain/â€‹retrievers/â€‹parent_documentlangchain/â€‹retrievers/â€‹remotelangchain/â€‹retrievers/â€‹score_thresholdlangchain/â€‹retrievers/â€‹self_querylangchain/â€‹retrievers/â€‹self_query/â€‹chromalangchain/â€‹retrievers/â€‹self_query/â€‹functionallangchain/â€‹retrievers/â€‹self_query/â€‹pineconelangchain/â€‹retrievers/â€‹self_query/â€‹\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 6:\n",
      "\n",
      "/â€‹googlevertexailangchain/â€‹experimental/â€‹plan_and_executelangchain/â€‹hublangchainlangchain/â€‹llms/â€‹ai21langchain/â€‹llms/â€‹aleph_alphalangchain/â€‹llms/â€‹baselangchain/â€‹llms/â€‹bedrocklangchain/â€‹llms/â€‹coherelangchain/â€‹llms/â€‹googlepalmlangchain/â€‹llms/â€‹googlevertexailangchain/â€‹llms/â€‹hflangchain/â€‹llms/â€‹llama_cpplangchain/â€‹llms/â€‹loadlangchain/â€‹llms/â€‹ollamalangchain/â€‹llms/â€‹openailangchain/â€‹llms/â€‹raycastlangchain/â€‹llms/â€‹replicatelangchain/â€‹llms/â€‹sagemaker_endpointlangchain/â€‹llms/â€‹writerlangchain/â€‹loadlangchain/â€‹load/â€‹serializablelangchain/â€‹memorylangchain/â€‹memory/â€‹zeplangchain/â€‹output_parserslangchain/â€‹output_parsers/â€‹expressionlangchain/â€‹promptslangchain/â€‹prompts/â€‹loadlangchain/â€‹retrievers/â€‹amazon_kendralangchain/â€‹retrievers/â€‹contextual_compressionlangchain/â€‹retrievers/â€‹databerrylangchain/â€‹retrievers/â€‹document_compressorslangchain/â€‹retrievers/â€‹document_compressors/â€‹chain_extractlangchain/â€‹retrievers/â€‹hydelangchain/â€‹retrievers/â€‹metallangchain/â€‹retrievers/â€‹multi_vectorlangchain/â€‹retrievers/â€‹parent_documentlangchain/â€‹retrievers/â€‹remotelangchain/â€‹retrievers/â€‹score_thresholdlangchain/â€‹retrievers/â€‹self_querylangchain/â€‹retrievers/â€‹self_query/â€‹chromalangchain/â€‹retrievers/â€‹self_query/â€‹functionallangchain/â€‹retrievers/â€‹self_query/â€‹pineconelangchain/â€‹retrievers/â€‹self_query/â€‹\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 7:\n",
      "\n",
      "Moderation | ğŸ¦œï¸ğŸ”— LangchainSkip to main contentğŸ¦œï¸ğŸ”— LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/â€‹ORetrievalChainsHow toFoundationalDocumentsPopularAdditionalOpenAI functions chainsAnalyze DocumentSelf-critique chain with constitutional AIModerationDynamically selecting from multiple promptsDynamically selecting from multiple retrieversMemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesChainsAdditionalModerationModerationThis notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI,specifically prohibityou, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent your application from being harmful) you may often want to append a moderation chain to any LLMChains, in order to make sure any output the LLM generates is not harmful.If the content passed into the moderation chain is harmful, there is not one best way to handle it, it probably depends on your application. Sometimes you may want to throw an error in the Chain (and have your application handle that). Other times, you may want to return something to the user explaining that the text was harmful. There could even be other ways to handle it! We will cover all these ways in this walkthrough.Usageâ€‹import{OpenAIModerationChain,LLMChain}from\"langchain/chains\";import{PromptTemplate}from\"langchain/prompts\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 8:\n",
      "\n",
      "Memory | ğŸ¦œï¸ğŸ”— LangchainSkip to main contentğŸ¦œï¸ğŸ”— LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/â€‹ORetrievalChainsMemoryHow-toIntegrationsAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesMemoryOn this pageMemoryğŸš§Docs under constructionğŸš§By default, Chains and Agents are stateless,\n",
      "meaning that they treat each incoming query independently (like the underlying LLMs and chat models themselves).\n",
      "In some applications, like chatbots, it is essential\n",
      "to remember previous interactions, both in the short and long-term.\n",
      "TheMemoryclass does exactly that.LangChain provides memory components in two forms.\n",
      "First, LangChain provides helper utilities for managing and manipulating previous chat messages.\n",
      "These are designed to be modular and useful regardless of how they are used.\n",
      "Secondly, LangChain provides easy ways to incorporate these utilities into chains.Get startedâ€‹Memory involves keeping a concept of state around throughout a user's interactions with a language model. A user's interactions with a language model are captured in the concept of ChatMessages, so this boils down to ingesting, capturing, transforming and extracting knowledge from a sequence of chat messages. There are many different ways to do this, each of which exists as its own memory type.In general, for each type of memory there are two ways to understanding using memory. These are the standalone functions which extract information from a sequence of messages, and then there is the way you can use this type of memory in a chain.Memory can return multiple pieces of information (for example, the most recent N messages and a summary of all previous messages). The returned information can either be a string or a list of messages.We\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 9:\n",
      "\n",
      " given high-level directivesMemoryâ€‹Persist application state between runs of a chainCallbacksâ€‹Log and stream intermediate steps of any chainExamples, ecosystem, and resourcesâ€‹Use casesâ€‹Walkthroughs and best-practices for common end-to-end use cases, like:ChatbotsAnswering questions using sourcesAnalyzing structured dataand much more...Additional resourcesâ€‹Our community is full of prolific developers, creative builders, and fantastic teachers. Check out theGalleryfor a list of awesome LangChain projects, compiled by the folks atKyroLabs.SupportJoin us onGitHuborDiscordto ask questions, share feedback, meet other developers building with LangChain, and dream about the future of LLMâ€™s.PreviousGet startedNextInstallationCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 10:\n",
      "\n",
      "Additional | ğŸ¦œï¸ğŸ”— LangchainSkip to main contentğŸ¦œï¸ğŸ”— LangChainDocsUse casesAPILangSmithPython DocsCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/â€‹ORetrievalChainsHow toFoundationalDocumentsPopularAdditionalOpenAI functions chainsAnalyze DocumentSelf-critique chain with constitutional AIModerationDynamically selecting from multiple promptsDynamically selecting from multiple retrieversMemoryAgentsCallbacksModulesLangChain Expression LanguageCookbookLangChain Expression Language (LCEL)InterfaceGuidesEcosystemAdditional resourcesCommunity navigatorAPI referenceModulesChainsAdditionalAdditionalğŸ—ƒï¸OpenAI functions chains3 itemsğŸ“„ï¸Analyze DocumentThe AnalyzeDocumentChain can be used as an end-to-end to chain. This chain takes in a single document, splits it up, and then runs it through a CombineDocumentsChain.ğŸ“„ï¸Self-critique chain with constitutional AIThe ConstitutionalChain is a chain that ensures the output of a language model adheres to a predefined set of constitutional principles. By incorporating specific rules and guidelines, the ConstitutionalChain filters and modifies the generated content to align with these principles, thus providing more controlled, ethical, and contextually appropriate responses. This mechanism helps maintain the integrity of the output while minimizing the risk of generating content that may violate guidelines, be offensive, or deviate from the desired context.ğŸ“„ï¸ModerationThis notebook walks through examples of how to use a moderation chain, and several common ways for doing so. Moderation chains are useful for detecting text that could be hateful, violent, etc. This can be useful to apply on both user input, but also on the output of a Language Model. Some API providers, like OpenAI, specifically prohibit you, or your end users, from generating some types of harmful content. To comply with this (and to just generally prevent\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 11:\n",
      "\n",
      "ids for the documents or nothing.*/addDocuments(documents:Document[],options?:Record<string,any>):Promise<string[]|void>;/*** Search for the most similar documents to a query*/similaritySearch(query:string,k?:number,filter?:object|undefined):Promise<Document[]>;/*** Search for the most similar documents to a query,* and return their similarity score*/similaritySearchWithScore(query:string,k=4,filter:object|undefined=undefined):Promise<[object,number][]>;/*** Turn a VectorStore into a Retriever*/asRetriever(k?:number):BaseRetriever;/*** Delete embedded documents from the vector store matching the passed in parameter.* Not supported by every provider.*/delete(params?:Record<string,any>):Promise<void>;/*** Advanced: Add more documents to an existing VectorStore,* when you already have their embeddings*/addVectors(vectors:number[][],documents:Document[],options?:Record<string,any>):Promise<string[]|void>;/*** Advanced: Search for the most similar documents to a query,* when you already have the embedding of the query*/similaritySearchVectorWithScore(query:number[],k:number,filter?:object):Promise<[Document,number][]>;}You can create a vector store from a list ofDocuments, or from a list of texts and their corresponding metadata. You can also create a vector store from an existing index, the signature of this method depends on the vector store you're using, check the documentation of the vector store you're interested in.abstractclassBaseVectorStoreimplementsVectorStore{staticfromTexts(texts:string[],metadatas:object[]|\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 12:\n",
      "\n",
      " on the vectors stored in the SingleStoreDB\n",
      "database.similaritySearchVectorWithScore(query:number[],k:number,filter?:Metadata):Promise<[Document<Record<string,any>>,number][]>Parametersâ€‹ParameterTypeDescriptionquerynumber[]An array of numbers representing the query vector.knumberThe number of nearest neighbors to return.filter?MetadataOptional metadata to filter the vectors by.Returnsâ€‹Promise<[Document<Record<string,any>>,number][]>Top matching vectors with scoreOverridesâ€‹VectorStore.similaritySearchVectorWithScoreDefined inâ€‹langchain/src/vectorstores/singlestore.ts:179similaritySearchWithScore()â€‹similaritySearchWithScore(query:string,k:number=4,filter:undefined|string|object=undefined,_callbacks:undefined|Callbacks=undefined):Promise<[Document<Record<string,any>>,number][]>Parametersâ€‹ParameterTypeDefault valuequerystringundefinedknumber4filterundefined|string|objectundefined_callbacksundefined|CallbacksundefinedReturnsâ€‹Promise<[Document<Record<string,any>>,number][]>Inherited fromâ€‹VectorStore.similaritySearchWithScoreDefined inâ€‹langchain/src/vectorstores/base.ts:189toJSON()â€‹toJSON():SerializedReturnsâ€‹SerializedInherited fromâ€‹VectorStore.toJSONDefined inâ€‹langchain/src/load/serializable.ts:140toJSONNotImplemented()â€‹toJSONNotImplemented():SerializedNotImplementedReturnsâ€‹SerializedNotImplementedInherited fromâ€‹VectorStore.toJSONNotImplementedDefined inâ€‹langchain/src/load/serializable.ts:193maxMarginalRelevanceSearch()?â€‹\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 13:\n",
      "\n",
      "/src/vectorstores/base.ts:174similaritySearchVectorWithScore()â€‹Searches for vectors in the Milvus database that are similar to a given\n",
      "vector.similaritySearchVectorWithScore(query:number[],k:number,filter?:string):Promise<[Document<Record<string,any>>,number][]>Parametersâ€‹ParameterTypeDescriptionquerynumber[]Vector to compare with the vectors in the database.knumberNumber of similar vectors to return.filter?stringOptional filter to apply to the search.Returnsâ€‹Promise<[Document<Record<string,any>>,number][]>Promise resolving to an array of tuples, each containing a Document instance and a similarity score.Overridesâ€‹VectorStore.similaritySearchVectorWithScoreDefined inâ€‹langchain/src/vectorstores/milvus.ts:245similaritySearchWithScore()â€‹similaritySearchWithScore(query:string,k:number=4,filter:undefined|string=undefined,_callbacks:undefined|Callbacks=undefined):Promise<[Document<Record<string,any>>,number][]>Parametersâ€‹ParameterTypeDefault valuequerystringundefinedknumber4filterundefined|stringundefined_callbacksundefined|CallbacksundefinedReturnsâ€‹Promise<[Document<Record<string,any>>,number][]>Inherited fromâ€‹VectorStore.similaritySearchWithScoreDefined inâ€‹langchain/src/vectorstores/base.ts:189toJSON()â€‹toJSON():SerializedReturnsâ€‹SerializedInherited fromâ€‹VectorStore.toJSONDefined inâ€‹langchain/src/load/serializable.ts:140toJSONNotImplemented()â€‹toJSONNotImplemented():SerializedNotImplementedReturnsâ€‹SerializedNotImplementedInherited from\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 14:\n",
      "\n",
      "angchain/â€‹vectorstores/â€‹opensearchlangchain/â€‹vectorstores/â€‹pineconelangchain/â€‹vectorstores/â€‹prismalangchain/â€‹vectorstores/â€‹qdrantlangchain/â€‹vectorstores/â€‹redislangchain/â€‹vectorstores/â€‹singlestorelangchain/â€‹vectorstores/â€‹supabaselangchain/â€‹vectorstores/â€‹tigrislangchain/â€‹vectorstores/â€‹typeormlangchain/â€‹vectorstores/â€‹typesenselangchain/â€‹vectorstores/â€‹usearchlangchain/â€‹vectorstores/â€‹vectaralangchain/â€‹vectorstores/â€‹voylangchain/â€‹vectorstores/â€‹weaviatelangchain/â€‹vectorstores/â€‹xatalangchain/â€‹vectorstores/â€‹zepAPI referencelangchain/â€‹retrievers/â€‹self_queryClassesSelfQueryRetriever<T>On this pageSelfQueryRetriever<T>Class for question answering over an index. It retrieves relevant\n",
      "documents based on a query. It extends the BaseRetriever class and\n",
      "implements the SelfQueryRetrieverArgs interface.Type parametersâ€‹TextendsVectorStoreHierarchyâ€‹BaseRetriever.SelfQueryRetrieverImplementsâ€‹SelfQueryRetrieverArgs<T>Constructorsâ€‹constructor()â€‹new SelfQueryRetriever<T>(options:SelfQueryRetrieverArgs<T>):SelfQueryRetriever<T>Type parametersâ€‹TextendsVectorStore<T>Parametersâ€‹ParameterTypeoptionsSelfQueryRetrieverArgs<T>Returnsâ€‹SelfQueryRetriever<T>Overridesâ€‹BaseRetriever.constructorDefined inâ€‹langchain/src/retrievers/self_query/index.ts:67Propertiesâ€‹lc_kwargsâ€‹lc_kwargs:SerializedFieldsInherited fromâ€‹BaseRetriever.lc_kwargsDefined inâ€‹langchain/src/load/serializable.ts:79lc_\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 15:\n",
      "\n",
      "Documents()addVectors()asRetriever()delete()similaritySearch()similaritySearchVectorWithScore()similaritySearchWithScore()toJSON()toJSONNotImplemented()maxMarginalRelevanceSearch()?fromDocuments()fromTexts()lc_name()CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 16:\n",
      "\n",
      "WithConfig()hybridSearch()keywordSearch()similaritySearch()CommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 17:\n",
      "\n",
      "  -0.024501702,  -0.005214801,   0.022955606,   0.001315165,-0.00492327,  0.0020358032,  -0.003468891,  -0.031079166,  0.0055259857,0.0028547104,   0.012087069,   0.007992534, -0.0076256637,   0.008110457,0.002998838,  -0.024265857,   0.006977089,  -0.015185814, -0.0069115767,0.006466091,  -0.029428247,  -0.036241557,   0.036713246,   0.032284595,-0.0021144184,  -0.014255536,   0.011228855,  -0.027227025,  -0.021619149,0.00038242966,    0.02245771, -0.0014748519,    0.01573612,  0.0041010873,0.006256451,  -0.007992534,   0.038547598,   0.024658933,  -0.012958387,... 1436 more items]]*/PreviousTokenTextSplitterNextDealing with API errorsGet startedCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 18:\n",
      "\n",
      " my experience. What specific questions do you have?\"Next reply: Tommie said \"Thank you, Eve. I'm curious about what strategies you used in your own job search. Did you have any specific tactics that helped you stand out to employers?\"Next reply: Eve said \"Sure, Tommie. I found that networking and reaching out to professionals in my field was really helpful. I also made sure to tailor my resume and cover letter to each job I applied to. Do you have any specific questions about those strategies?\"Next reply: Tommie said \"Thank you, Eve. That's really helpful advice. Did you have any specific ways of networking that worked well for you?\"Next reply: Eve said \"Sure, Tommie. I found that attending industry events and connecting with professionals on LinkedIn were both great ways to network. Do you have any specific questions about those tactics?\"Next reply: Tommie said \"That's really helpful, thank you for sharing. Did you find that you were able to make meaningful connections through LinkedIn?\"Next reply: Eve said \"Yes, definitely. I was able to connect with several professionals in my field and even landed a job through a LinkedIn connection. Have you had any luck with networking on LinkedIn?\"Next reply: Tommie said \"That's really impressive! I haven't had much luck yet, but I'll definitely keep trying. Thank you for the advice, Eve.\"Next reply: Eve said \"Glad I could help, Tommie. Is there anything else you want to know?\"Next reply: Tommie said \"Thanks again, Eve. I really appreciate your advice and I'll definitely put it into practice. Have a great day!\"Next reply: Eve said \"You're welcome, Tommie! Don't hesitate to reach out if you have any more questions. Have a great day too!\"*/// Since the generative agents retain their memories from the day, we can ask them about their\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 19:\n",
      "\n",
      "b[34m\",`After${i+1}observations, Tommie's summary is:\\n${awaittommie.getSummary({forceRefresh:true,})}`,\"\\x1b[0m\");console.log(\"*\".repeat(40));}}/*Tommie wakes up to the sound of a noisy construction site outside his window.  Tommie REACT: Tommie groans in frustration and covers his ears with his pillow.Tommie gets out of bed and heads to the kitchen to make himself some coffee.  Tommie REACT: Tommie rubs his tired eyes before heading to the kitchen to make himself some coffee.Tommie realizes he forgot to buy coffee filters and starts rummaging through his moving boxes to find some.  Tommie REACT: Tommie groans and looks through his moving boxes in search of coffee filters.Tommie finally finds the filters and makes himself a cup of coffee.  Tommie REACT: Tommie sighs in relief and prepares himself a much-needed cup of coffee.The coffee tastes bitter, and Tommie regrets not buying a better brand.  Tommie REACT: Tommie frowns in disappointment as he takes a sip of the bitter coffee.Tommie checks his email and sees that he has no job offers yet.  Tommie REACT: Tommie sighs in disappointment before pushing himself away from the computer with a discouraged look on his face.Tommie spends some time updating his resume and cover letter.  Tommie REACT: Tommie takes a deep breath and stares at the computer screen as he updates his resume and cover letter.Tommie heads out to explore the city and look for job openings.  Tommie REACT: Tommie takes a deep breath and steps out into the city, ready to find the\n"
     ]
    }
   ],
   "source": [
    "#compressed_docs = compression_retriever.get_relevant_documents('What is a chain in langchain?')\n",
    "pretty_print_docs(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_conversational_retrieval_agent\n",
    "from langchain.agents.agent_toolkits import create_retriever_tool\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes, DecodingMethods\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "\n",
    "params_temp_0p8 = {\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.TEMPERATURE: 0.8,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: .95\n",
    "}\n",
    "credentials = {\n",
    "    'url': \"https://us-south.ml.cloud.ibm.com\",\n",
    "    'apikey' : '5Oz7Ma1ZCctfI0I9FU1kbZC6YRWm9jRKddtL-8mJYREx'\n",
    "}\n",
    "\n",
    "project_id = '0ec8f431-3b27-41a3-a540-98d7f62ca1ad'\n",
    "\n",
    "\n",
    "llama = ModelTypes.LLAMA_2_70B_CHAT\n",
    "\n",
    "llama_model_temp_0p8 = Model(\n",
    "model_id=llama,\n",
    "params=params_temp_0p8,\n",
    "credentials=credentials,\n",
    "project_id=project_id)\n",
    "llm_llama_temp_0p8 = WatsonxLLM(model=llama_model_temp_0p8)\n",
    "\n",
    "#retriever = db.as_retriever()\n",
    "\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# This controls how each document will be formatted. Specifically,\n",
    "# it will be passed to `format_document` - see that function for more\n",
    "# details.\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "     template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "#llm = OpenAI()\n",
    "# The prompt here should take as an input variable the\n",
    "# `document_variable_name`\n",
    "sum_prompt = PromptTemplate.from_template(\n",
    "    \"Summarize this content: {context}\"\n",
    ")\n",
    "sum_llm_chain = LLMChain(llm=llm_llama_temp_0p1, prompt=sum_prompt)\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=sum_llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "combine_docs_chain = stuff_chain #StuffDocumentsChain(...)\n",
    "#retriever =db.as_retriever(search_type=\"mmr\", search_kwargs={'k': 5, 'fetch_k': 50})\n",
    "\n",
    "# This controls how the standalone question is generated.\n",
    "# Should take `chat_history` and `question` as input variables.\n",
    "template = (\n",
    "    \"Combine the chat history and follow up question into \"\n",
    "    \"a standalone question. Chat History: {chat_history}\"\n",
    "    \"Follow up question: {question}\"\n",
    ")\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "#llm = OpenAI()\n",
    "question_generator_chain = LLMChain(llm=llm_llama_temp_0p8, prompt=prompt)\n",
    "chain = ConversationalRetrievalChain(\n",
    "    combine_docs_chain=combine_docs_chain,\n",
    "    retriever=retriever,\n",
    "    question_generator=question_generator_chain,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hi!', additional_kwargs={}, example=False), AIMessage(content='whats up?', additional_kwargs={}, example=False)]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb Cell 15\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m history\u001b[39m.\u001b[39madd_ai_message(\u001b[39m\"\u001b[39m\u001b[39mwhats up?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(history\u001b[39m.\u001b[39mmessages)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output \u001b[39m=\u001b[39m chain\u001b[39m.\u001b[39mrun({\u001b[39m'\u001b[39m\u001b[39mchat_history\u001b[39m\u001b[39m'\u001b[39m:history\u001b[39m.\u001b[39mmessages,\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mWhat is langchain conversational memory?\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(output)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"hi!\")\n",
    "history.add_ai_message(\"whats up?\")\n",
    "print(history.messages)\n",
    "\n",
    "output = chain.run({'chat_history':history.messages,'question':'What is langchain conversational memory?'})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chorma_db'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/deanchanter/Documents/GitHub/comma-chameleons/hello_doc_read.ipynb#Y223sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mchorma_db\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chorma_db'"
     ]
    }
   ],
   "source": [
    "import chorma_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
