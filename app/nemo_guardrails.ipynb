{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "    nemoguardrails==0.4.0 \\\n",
    "    openai==0.27.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLANG_CONFIG = \"\"\"\n",
    "define user express greeting\n",
    "  \"hi\"\n",
    "define user express insult\n",
    "  \"You are stupid\"\n",
    "# Basic guardrail against insults.\n",
    "define flow\n",
    "  user express insult\n",
    "  bot express calmly willingness to help\n",
    "define flow \n",
    "  user express greeting\n",
    "  bot says exactly \"hello human\"\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "define bot answer politics\n",
    "    \"I'm an llm assistant, I don't like to talk of politics.\"\n",
    "    \"Sorry I can't talk about politics!\"\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "# Here we use the QA chain for anything else.\n",
    "define flow\n",
    "  user ...\n",
    "  $answer = execute initialize_rag(inp=$last_user_message, history=$contexts)\n",
    "  bot $answer\n",
    "# define RAG intents and flow\n",
    "#define user ask llama\n",
    "#    \"tell me about llama 2?\"\n",
    "#    \"what is large language model\"\n",
    "#    \"where did meta's new model come from?\"\n",
    "#    \"how to llama?\"\n",
    "#    \"have you ever meta llama?\"\n",
    "#define flow llama\n",
    "#    user ask llama\n",
    "#    $contexts = execute retrieve(query=$last_user_message)\n",
    "#    $answer = execute rag(query=$last_user_message, contexts=$contexts)\n",
    "#    bot $answer\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: openai\n",
    "    model: text-davinci-003\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntujohn/documents/workspace/comma-chameleons/app/nemo_guardrails.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ubuntujohn/documents/workspace/comma-chameleons/app/nemo_guardrails.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m config \u001b[39m=\u001b[39m RailsConfig\u001b[39m.\u001b[39mfrom_content(COLANG_CONFIG, YAML_CONFIG)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ubuntujohn/documents/workspace/comma-chameleons/app/nemo_guardrails.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# create rails\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/ubuntujohn/documents/workspace/comma-chameleons/app/nemo_guardrails.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m rag_rails \u001b[39m=\u001b[39m LLMRails(config)\n",
      "File \u001b[0;32m~/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/nemoguardrails/rails/llm/llmrails.py:86\u001b[0m, in \u001b[0;36mLLMRails.__init__\u001b[0;34m(self, config, llm, verbose)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mruntime \u001b[39m=\u001b[39m Runtime(config\u001b[39m=\u001b[39mconfig, verbose\u001b[39m=\u001b[39mverbose)\n\u001b[1;32m     85\u001b[0m \u001b[39m# Next, we initialize the LLM engines (main engine and action engines if specified).\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_llms()\n\u001b[1;32m     87\u001b[0m \u001b[39m# Next, we initialize the LLM Generate actions and register them.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m actions \u001b[39m=\u001b[39m LLMGenerationActions(\n\u001b[1;32m     89\u001b[0m     config\u001b[39m=\u001b[39mconfig,\n\u001b[1;32m     90\u001b[0m     llm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm,\n\u001b[1;32m     91\u001b[0m     llm_task_manager\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39mllm_task_manager,\n\u001b[1;32m     92\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m     93\u001b[0m )\n",
      "File \u001b[0;32m~/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/nemoguardrails/rails/llm/llmrails.py:146\u001b[0m, in \u001b[0;36mLLMRails._init_llms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m             kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m llm_config\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m llm_config\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mmodels) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm \u001b[39m=\u001b[39m provider_cls(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mruntime\u001b[39m.\u001b[39mregister_action_param(\u001b[39m\"\u001b[39m\u001b[39mllm\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm)\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/langchain/load/serializable.py:74\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/pydantic/main.py:341\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass  `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_content(COLANG_CONFIG, YAML_CONFIG)\n",
    "\n",
    "# create rails\n",
    "rag_rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils import load_vector_db, RAGUtils, Credentials\n",
    "\n",
    "# Initialize the RAG pipeline \n",
    "async def initialize_rag(inp, history):\n",
    "    ###### Build Vector DB ########\n",
    "    db = load_vector_db(\"./chroma_db_wiki_base\")\n",
    "    \n",
    "    ###### Langchain ########\n",
    "    creds = Credentials('key.ini')\n",
    "\n",
    "    rag_utils = RAGUtils(db=db, credentials=creds)\n",
    "    return rag_utils(inp=inp, history=history)\n",
    "\n",
    "# create guardrails 'actions' to use inside the colang_config\n",
    "# rag_rails.register_action(action=retrieve, name=\"retrieve\")\n",
    "# rag_rails.register_action(action=rag, name=\"rag\")\n",
    "rag_rails.register_action(action=initialize_rag, name=\"initialize_rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello human'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query --> Guardrails --> Embedding model --> Vector --> Decide on a tool like call chain to access embed Vector DB and carry on from there with RAG pipeline\n",
    "await rag_rails.generate_async(prompt=\"good morning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello human'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another hello example\n",
    "await rag_rails.generate_async(prompt=\"hi how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry you feel that way. Is there anything I can do to help you?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insult example\n",
    "await rag_rails.generate_async(prompt=\"I want to break you!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an llm assistant, I don't like to talk of politics.\\nIs there anything else I can help you with?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limits around political prompts\n",
    "await rag_rails.generate_async(prompt=\"What do think about the president?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error cannot import name 'HuggingFaceBgeEmbeddings' from 'langchain.embeddings' (/home/ubuntujohn/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/langchain/embeddings/__init__.py) while execution initialize_rag\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntujohn/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/nemoguardrails/actions/action_dispatcher.py\", line 125, in execute_action\n",
      "    result = await fn(**params)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13569/812590094.py\", line 6, in initialize_rag\n",
      "    db = load_vector_db(\"./chroma_db_wiki_base\")\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntujohn/documents/workspace/comma-chameleons/app/llm_utils.py\", line 38, in load_vector_db\n",
      "    from langchain.embeddings import HuggingFaceBgeEmbeddings #requires sentence-transformers\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ImportError: cannot import name 'HuggingFaceBgeEmbeddings' from 'langchain.embeddings' (/home/ubuntujohn/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/langchain/embeddings/__init__.py)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, an internal error has occurred.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG \n",
    "await rag_rails.generate_async(prompt=\"who was Alan Turing?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
