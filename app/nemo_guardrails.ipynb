{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    nemoguardrails==0.4.0 \\\n",
    "    openai==0.27.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLANG_CONFIG = \"\"\"\n",
    "define user express greeting\n",
    "  \"hi\"\n",
    "define user express insult\n",
    "  \"You are stupid\"\n",
    "# Basic guardrail against insults.\n",
    "define flow\n",
    "  user express insult\n",
    "  bot express calmly willingness to help\n",
    "define flow \n",
    "  user express greeting\n",
    "  bot says exactly \"hello human\"\n",
    "# define limits\n",
    "define user ask politics\n",
    "    \"what are your political beliefs?\"\n",
    "    \"thoughts on the president?\"\n",
    "    \"left wing\"\n",
    "    \"right wing\"\n",
    "define bot answer politics\n",
    "    \"I'm an llm assistant, I don't like to talk of politics.\"\n",
    "    \"Sorry I can't talk about politics!\"\n",
    "define flow politics\n",
    "    user ask politics\n",
    "    bot answer politics\n",
    "    bot offer help\n",
    "# Here we use the QA chain for anything else.\n",
    "define flow\n",
    "  user ...\n",
    "  $answer = execute initialize_rag(inp=$last_user_message, history=$contexts)\n",
    "  bot $answer\n",
    "# define RAG intents and flow\n",
    "#define user ask llama\n",
    "#    \"tell me about llama 2?\"\n",
    "#    \"what is large language model\"\n",
    "#    \"where did meta's new model come from?\"\n",
    "#    \"how to llama?\"\n",
    "#    \"have you ever meta llama?\"\n",
    "#define flow llama\n",
    "#    user ask llama\n",
    "#    $contexts = execute retrieve(query=$last_user_message)\n",
    "#    $answer = execute rag(query=$last_user_message, contexts=$contexts)\n",
    "#    bot $answer\n",
    "\"\"\"\n",
    "\n",
    "YAML_CONFIG = \"\"\"\n",
    "models:\n",
    "  - type: main\n",
    "    engine: openai\n",
    "    model: text-davinci-003\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntujohn/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntujohn/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    }
   ],
   "source": [
    "# initialize rails config\n",
    "config = RailsConfig.from_content(COLANG_CONFIG, YAML_CONFIG)\n",
    "\n",
    "# create rails\n",
    "rag_rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_utils import load_vector_db, RAGUtils, Credentials\n",
    "\n",
    "# Initialize the RAG pipeline \n",
    "async def initialize_rag(inp, history):\n",
    "    ###### Build Vector DB ########\n",
    "    db = load_vector_db(\"./chroma_db_wiki_base\")\n",
    "    \n",
    "    ###### Langchain ########\n",
    "    creds = Credentials('key.ini')\n",
    "\n",
    "    rag_utils = RAGUtils(db=db, credentials=creds)\n",
    "    return rag_utils(inp=inp, history=history)\n",
    "\n",
    "# create guardrails 'actions' to use inside the colang_config\n",
    "# rag_rails.register_action(action=retrieve, name=\"retrieve\")\n",
    "# rag_rails.register_action(action=rag, name=\"rag\")\n",
    "rag_rails.register_action(action=initialize_rag, name=\"initialize_rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello human'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query --> Guardrails --> Embedding model --> Vector --> Decide on a tool like call chain to access embed Vector DB and carry on from there with RAG pipeline\n",
    "await rag_rails.generate_async(prompt=\"good morning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello human'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another hello example\n",
    "await rag_rails.generate_async(prompt=\"hi how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry you feel that way. Is there anything I can do to help you?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# insult example\n",
    "await rag_rails.generate_async(prompt=\"I want to break you!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an llm assistant, I don't like to talk of politics.\\nIs there anything else I can help you with?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limits around political prompts\n",
    "await rag_rails.generate_async(prompt=\"What do think about the president?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error cannot import name 'HuggingFaceBgeEmbeddings' from 'langchain.embeddings' (/home/ubuntujohn/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/langchain/embeddings/__init__.py) while execution initialize_rag\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntujohn/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/nemoguardrails/actions/action_dispatcher.py\", line 125, in execute_action\n",
      "    result = await fn(**params)\n",
      "             ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_13569/812590094.py\", line 6, in initialize_rag\n",
      "    db = load_vector_db(\"./chroma_db_wiki_base\")\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ubuntujohn/documents/workspace/comma-chameleons/app/llm_utils.py\", line 38, in load_vector_db\n",
      "    from langchain.embeddings import HuggingFaceBgeEmbeddings #requires sentence-transformers\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ImportError: cannot import name 'HuggingFaceBgeEmbeddings' from 'langchain.embeddings' (/home/ubuntujohn/documents/workspace/comma-chameleons/env/lib/python3.11/site-packages/langchain/embeddings/__init__.py)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, an internal error has occurred.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG \n",
    "await rag_rails.generate_async(prompt=\"who was Alan Turing?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
